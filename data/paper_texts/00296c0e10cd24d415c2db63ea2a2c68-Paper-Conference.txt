Path following algorithms for ‚Ñì2-regularized
M-estimation with approximation guarantee
Yunzhang Zhu
Department of Statistics
Ohio State University
Columbus, OH 43015
zhu.219@osu.edu
Renxiong Liu‚àó
Statistics and Data Science team
Nokia Bell Labs
Murray Hill, NJ 07974
renxiong.liu@nokia-bell-labs.com
Abstract
Many modern machine learning algorithms are formulated as regularized M-
estimation problems, in which a regularization (tuning) parameter controls a trade-
off between model fit to the training data and model complexity. To select the ‚Äúbest‚Äù
tuning parameter value that achieves a good trade-off, an approximated solution
path needs to be computed. In practice, this is often done through selecting a grid
of tuning parameter values and solving the regularized problem at the selected grid
points. However, given any desired level of accuracy, it is often not clear how to
choose the grid points and also how accurately one should solve the regularized
problems at the selected gird points, both of which can greatly impact the overall
amount of computation. In the context of ‚Ñì2-regularized M-estimation problem,
we propose a novel grid point selection scheme and an adaptive stopping criterion
for any given optimization algorithm that produces an approximated solution path
with approximation error guarantee. Theoretically, we prove that the proposed
solution path can approximate the exact solution path to arbitrary level of accuracy,
while saving the overall computation as much as possible. Numerical results also
corroborate our theoretical analysis.
1
Introduction
Modern machine learning algorithms are often formulated as regularized M-minimization problems,
Œ∏(Œª) = arg min
Œ∏
Ln(Œ∏) + Œªp(Œ∏) ,
(1)
where Ln(Œ∏) denotes an empirical loss function, p(Œ∏) denotes a regularization function, and Œª is a
tuning parameter that controls the trade-off between model fit and model complexity. Varying the
tuning parameter leads to a collection of models, among which one of them may be chosen as the
final model. This requires solving a collection of related optimization problems, whose solutions are
often referred to as the solution path (We refer readers to Figure S4 and S5 in the supplementary
material for some examples of solution paths).
Very often the exact solution path Œ∏(Œª) can not be computed, and path-following algorithms are
usually used to obtain a sequence of solutions at some selected grid points to produce an approximated
solution path. Existing path-following algorithms [see, e.g., Hastie et al., 2004, 2007] typically
choose a set of equally-spaced grid points (often on a log-scale), and choose certain algorithm to solve
the minimization problem at the selected grid points. Warm-start strategy is often used to leverage
the fact that the minimizers at two consecutive grid points may be close. However, there is a paucity
of literature on how to choose these grid points and how accurately one should solve the optimization
‚àóWork done when Renxiong Liu is a PhD student at the Ohio State University.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).
problem at the selected grid points, both of which would influence the overall computational cost and
approximation accuracy.
In the context of ‚Ñì2-regularized M-estimation where p(Œ∏) = (1/2)‚à•Œ∏‚à•2
2, we propose a novel grid
point selection scheme and an associated stopping criterion at each grid point for any optimization
method. The proposal is motivated by first proving that the approximation error of a solution path is
governed by two important quantities, one depending on how finely the grid points are spaced and
the other depending on how accurately one solves the regularized problem at the selected grid points.
We refer these two quantities as interpolation error and optimization error. Given any optimization
algorithm (that solves a single regularized problem), a stopping criterion can be constructed so that
the optimization error is comparable to the interpolation error, which ensures that there is no wasteful
computation at each selected grid point. Then a novel grid point selection scheme is constructed to
balance the approximation errors over the entire solution path.
Theoretically, we first show that the approximation error of the proposed solution path can be upper
bounded by the sum of the interpolation error and the optimization error. Given any optimization
algorithm, we show that the proposed stopping criterion ensures that the optimization error is
dominated by the interpolation error (see Theorem 1). We then establish a global approximation-error
bound for the solution path (see Theorem 2). Optimizing this global approximation-error bound
by using the proposed grid point selection scheme, we also prove that the proposed solution path
can approximate the exact solution path to an arbitrary level of accuracy (see Theorem 3). Finally,
we establish that the total number of grid points required for the proposed algorithm to achieve an
œµ-suboptimality is at most O(œµ‚àí1/2) (see Theorem 4).
Path-following techniques have been used in many optimization algorithms. For instance, it is
used in the interior point methods, which is one of most popular algorithm to solve general convex
optimization problems [see, e.g., Chapter 1.3 of Nesterov and Nemirovskii, 1993]. However, the
focus is often on the limit of the path rather than the entire solution path. In statistics/machine
learning literature, Osborne [1992] and Osborne et al. [2000] applied the homotopy techniques to
obtain piecewise linear solution paths for quantile regression and LASSO, respectively. Later Efron
et al. [2004], Hastie et al. [2004], and Rosset and Zhu [2007] used similar ideas to derive the solution
path for a family of regularized M-estimation problems. Subsequent developments include Friedman
et al. [2007], Hoefling [2010], Arnold and Tibshirani [2016], among many others. Path following
algorithms for more general types of ‚Ñì1-reguarlization are also available. For example, Tibshirani
and Taylor [2011] used dual path algorithm to compute the solution path of a generalized LASSO
problem. In the context of order weight ‚Ñì1 model, Bao et al. [2019] proposed an efficient approximate
solution path algorithm by using primal-dual method. These works often leverage the piecewise
linearity of the solution path so that an exact path-following algorithm can be explicitly derived. For
situations where the solution paths are not piecewise linear, a path-following algorithm based on
Newton method was considered in Rosset [2004]. However, Rosset [2004] used a simple grid point
selection scheme and only established pointwise closeness to the solution path. Subsequent research
on situations where the solution paths are not piecewise linear includes the work of Garrigues and
Ghaoui [2008], which introduced a homotopy algorithm tailored for solving the LASSO problem
using online (sequential) observations, and the work of Qu et al. [2022], which approximates the
piecewise smooth path in self-paced learning for generalized self-paced regularizer.
To the best of our knowledge, there is a paucity of research on the impact of grid points selection and
how to set stopping criterion to save computation. One line of related works use a piecewise constant
interpolation scheme to approximate the solution path to an arbitrary level of accuracy [see, e.g.,
Giesen et al., 2010, 2012, 2014, Ndiaye et al., 2019]. Most relevant to our work here are the works
of Giesen et al. [2012] and Ndiaye et al. [2019]. Giesen et al. [2012] proposed a generic algorithm
to compute a piecewise constant approximate solution path for a family of optimization problems
parameterized concavely2 in a single hyperparameter. They showed that the number of grid points
needed to achieve an œµ suboptimality is at most O(1/‚àöœµ). However, their proposed method relies on
the existence of a problem-dependent oracle, which for any chosen hyperparameter produces both an
œµ-approximate solution at that hyperparameter and a concave polynomial that is used to compute the
step size. Except for special examples like standard SVM, it not clear how to implement the oracle
for general problems.
2It requires that the object function is concave with respect to the hyperparameter for any fixed value of
parameters. This includes most regularized machine learning problems as special cases.
2
In view of these limitations, Ndiaye et al. [2019] considered regularized convex optimization problems
and proposed general strategies to choose grid points based on primal-dual methods. Again, a
piecewise constant path is constructed to approximate the solution path. However, their approach can
only be applied to a special class of loss functions that are composition of a ‚Äúsimple‚Äù function (whose
conjugate can be analytically derived) and affine functions. Moreover, the choice of the grid points
depends on some convexity and smoothness parameters associated with the loss, which either are not
available except for some special class of functions or require careful tuning due to their great impact
on grid points selection. Ndiaye et al. [2019] also showed that to achieve œµ suboptimality, the number
of grid points required is O(1/
d‚àöœµ) for uniformly convex loss of order d [Bauschke et al., 2011] and
O(1/‚àöœµ) for generalized self-concordant functions [Sun and Tran-Dinh, 2017]. In our paper, we
show that the number of grid points needed is O(1/‚àöœµ) for general differentiable convex loss (see
Theorem 4). Compared to Ndiaye et al. [2019], we also demonstrate through simulations that our
method produces better approximated solution paths under a fixed computational budget (see Section
4). Moreover, our method can also be applicable to loss functions that are possibly nonconvex (see
Section 3).
We also note that Proposition 38 of Ndiaye [2018] shows that under some regularity conditions œµ-
suboptimality along the entire solution path can be achieved by using a piecewise linear interploation.
However, it not clear how these conditions can be used to construct an algorithm with approximation
guarantee. Our proposed algorithm, on the other hand, provides an easily implementable grid point
selection scheme and an associated stopping criterion at each grid point, which together ensures that
œµ-suboptimality can be attained through a piecewise linear interploation.
More recently, in the context of ‚Ñì2-regularized convex optimization problems, Zhu and Liu [2021]
proposed two path-following algorithms by using Newton method and gradient descent method as the
basis algorithm seperately, and discuss the grid point selection scheme for each algorithm based on
the piecewise linear interpolation. Our paper differs from Zhu and Liu [2021] in twofold. First, the
grid-point selection scheme in Zhu and Liu [2021] depends on the basis algorithm. By contrast, in
our paper any basis algorithm can be applied at any chosen grid point as long as the stopping criterion
is satisfied by the basis algorithm. Next, the path-following algorithms by Zhu and Liu [2021] can
only be applied to convex optimization problems, while in this article we also consider loss functions
that are possibly nonconvex (see Section 3).
The rest of the paper is organized as follows. Section 2 introduces the proposed path following
algorithm and establish its global approximation-error bound. Section 3 considers an extension
to nonconvex empirical loss. In Section 4, we compare the the proposed scheme to a standard
path-following scheme through a simulated study using ridge regression and ‚Ñì2-regularized logistic
regression. We close the article with some remarks in Section 5. All proofs are included in the
supplementary material.
2
Path following algorithms
We consider an ‚Ñì2-regularized M-estimation problem. More specifically, denote by Ln(Œ∏) some
empirical loss function, where Œ∏ ‚ààRp denotes a parameter vector. We consider the solution path of
an ‚Ñì2-regularized M-estimation problem:
Œ∏(t) = arg min
Œ∏‚ààRp

(et ‚àí1) ¬∑ Ln(Œ∏) + (1/2) ¬∑ ‚à•Œ∏‚à•2
2
	
.
(2)
Note that as the tuning parameter t varies from 0 to ‚àû, the solution Œ∏(t) varies from 0 to a minimizer
of Ln(Œ∏). We also remark that the choice of et ‚àí1 is not essential. In fact, it can be replaced
by any function C(t) that is a strictly increasing and differentiable function, with C(0) = 0 and
limt‚Üí‚àûC(t) = ‚àû. The solution path produced should be independent of C(t). Moreover, we can
show that the ‚Ñì2 norm of the solutions is an increasing function of t, and the solution Œ∏(t) converges
to the minimum ‚Ñì2 norm minimizer of Ln(Œ∏) (if it is finite) as t goes to infinity (see Corollary S1 in
the supplementary material).
Suppose that the goal is to approximate the solution path Œ∏(t) over a given interval [0, tmax) for some
tmax ‚àà(0, ‚àû], where we allow tmax = ‚àû. Given a set of grid points 0 < t1 < ¬∑ ¬∑ ¬∑ < tN < ‚àû, and
approximated solutions {Œ∏k}N
k=1 at these grid points, we construct an approximated solution path
over [0, tmax) through linear interpolation. More specifically, we define a piecewise linear solution
3
path ÀúŒ∏(t) as follows
ÀúŒ∏(t) =
tk+1‚àít
tk+1‚àítk Œ∏k +
t‚àítk
tk+1‚àítk Œ∏k+1
for any t ‚àà[tk, tk+1], k = 0, 1, . . . , N ‚àí1 ,
ÀúŒ∏(t) = Œ∏N
for any tN < t ‚â§tmax if tN < tmax ,
(3)
where t0 = 0 and Œ∏0 = 0. This defines an approximated solution path ÀúŒ∏(t) for any t ‚àà[0, tmax],
which interpolates the approximated solutions at the selected grid points. In view of this definition, we
may also assume that tN‚àí1 ‚â§tmax, because we do not need ÀúŒ∏(t) over t ‚àà[tN‚àí1, tN] if tN‚àí1 > tmax.
To assess how well ÀúŒ∏(t) approximates Œ∏(t), we use the function-value suboptimality of the solution
paths defined by sup0‚â§t‚â§tmax{ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))}, where ft(Œ∏) := (1 ‚àíe‚àít)Ln(Œ∏) + e‚àít(‚à•Œ∏‚à•2
2/2)
is a scaled version of the objective function in (2). In what follows, we call sup0‚â§t‚â§tmax{ft(ÀúŒ∏(t)) ‚àí
ft(Œ∏(t))} the global approximation error of ÀúŒ∏(t), and suptk‚â§t‚â§tk+1{ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))} the local
approximation error of ÀúŒ∏(t) over [tk, tk+1].
Intuitively, how well ÀúŒ∏(t) approximates Œ∏(t) depends on how finely spaced the grid points are and
the quality of the solutions at the selected grid points (i.e., Œ∏k‚Äôs). Our first main result relates the
local approximation error of ÀúŒ∏ over [tk, tk+1] to the choice of the grid points and the accuracy of the
approximated solutions at the selected grid points measured by the size of the gradients ‚à•‚àáftk(Œ∏k)‚à•2,
where we assume that Ln(Œ∏) is differentiable.
Theorem 1. Assume that Ln(Œ∏) is a differentiable convex function. Let gk := ‚àáftk(Œ∏k) = (1 ‚àí
e‚àítk)‚àáLn(Œ∏k) + e‚àítkŒ∏k denote the scaled gradient at Œ∏k. For any 0 < t1 < t2 < ¬∑ ¬∑ ¬∑ < tN < ‚àû,
we have that
sup
t‚àà[0,t1]
n
ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))
o
‚â§max
 et1‚à•g1‚à•2
2, ‚à•Œ∏1‚à•2
2

+ et1(1 ‚àíe‚àít1)2
2
‚à•‚àáLn(0)‚à•2
2 ,
(4)
sup
t‚àà[tk,tk+1]
n
ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))
o
‚â§etk+1 max
(1 ‚àíe‚àítk+1
1 ‚àíe‚àítk
2
‚à•gk‚à•2
2, ‚à•gk+1‚à•2
2
)
+ (e‚àítk ‚àíe‚àítk+1)2 max
 etk+1‚à•Œ∏k‚à•2
2
(1 ‚àíe‚àítk)2 ,
etk‚à•Œ∏k+1‚à•2
2
(1 ‚àíe‚àítk+1)2

(5)
for any k = 1, . . . , N ‚àí1. If we further assume that ‚à•Œ∏(tmax)‚à•2 < ‚àû, then we have that
sup
tN<t‚â§tmax
n
ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))
o
‚â§
etN
2(1 ‚àíe‚àítN )‚à•gN‚à•2
2 +
1
2(etN ‚àí1)‚à•Œ∏(tmax)‚à•2
2
(6)
when tN < tmax.
The proof of Theorem 1 starts with relating the local approximation error to the suboptimality of Œ∏k
and Œ∏k+1 at some t ‚àà[tk, tk+1]. The suboptimality can then further bounded by the square norm
of the grandient leveraging the fact that the objective function is e‚àít-strongly convex. Finally, a
triangular inequality is used to bound the norm of the grandient by quantities that depend on ‚à•gk‚à•2
and ‚à•Œ∏k‚à•2.
We can see from Theorem 1 that the upper bounds for the local approximation error in (4)‚Äì(6) consist
of two terms, with the first one (depending on gk) being algorithm dependent and the other term
stemming from interpolation. We call them optimization error and interpolation error, respectively.
Note that the optimization error is roughly of order etk‚à•gk‚à•2
2, which again depends on how accurate
the solutions Œ∏k are at the selected grid points. And the interpolation error is essentially independent
of the quality of the solutions Œ∏k‚Äôs, as the interpolation error only depends on the spacing of the
grid points and the norm of the solutions along the solution path (typically ‚à•Œ∏k‚à•2 = O(‚à•Œ∏(tk)‚à•2),
c.f., Lemma S2 in the supplementary material). In other words, the interpolation error is irreducible
once the grid points are chosen, while the optimization error does depend on the algorithm and it
can be pushed to be arbitrarily small if we run the algorithm long enough at each grid point. In
this sense, if the goal is to minimize the local approximation error, then both the grid points and
optimization algorithm should be designed carefully to strike a balance between these two errors
to minimize the overall computations. For instance, it would be wasteful to have the optimization
error smaller than the interpolation error, because the additional computation would not improve the
overall approximation error (at least in order).
4
Motivated by this observation and the local approximation-error bounds in Theorem 1, we propose a
novel stopping criterion scheme at the selected grid points for any general optimization algorithm. In
particular, for any given algorithm that takes an initializer and solves the problem at grid point tk+1,
we can run the algorithm initialized at Œ∏k until the optimization error is smaller than the interpolation
error for Œ∏k+1, that is
etk+1 max
(1 ‚àíe‚àítk+1
1 ‚àíe‚àítk
2
‚à•‚àáftk(Œ∏k)‚à•2
2, ‚à•‚àáftk(Œ∏k+1)‚à•2
2
)
|
{z
}
optimization error
‚â≤(e‚àítk ‚àíe‚àítk+1)2 max
 etk+1‚à•Œ∏k‚à•2
2
(1 ‚àíe‚àítk)2 ,
etk‚à•Œ∏k+1‚à•2
2
(1 ‚àíe‚àítk+1)2

|
{z
}
interpolation error
,
(7)
where the LHS is the optimization error and the RHS is the interpolation error in the bounds in
Theorem 1. This condition can be further simplified, under which the upper bounds in Theorem 1 can
also be further simplified and combined so that we can bound the global approximation error of ÀúŒ∏(t).
This is established in the following theorem.
Theorem 2. Assume that Ln(Œ∏) is a differentiable and convex. Suppose that 2‚àí1Œ±k ‚â§Œ±k+1 ‚â§2Œ±k,
Œ±k ‚â§Œ±max for some Œ±max ‚â§ln(2), and
‚à•‚àáftk(Œ∏k)‚à•2 ‚â§C0
(eŒ±k ‚àí1)
(etk ‚àí1) ‚à•Œ∏k‚à•2; 1 ‚â§k ‚â§N
(8)
for some C0 ‚â§1/4. Denote
A = 4(1 + C2
0(eŒ±max + eŒ±max/2 + 1)2) ,
B = (eŒ±1 ‚àí1)2‚à•‚àáLn(0)‚à•2
2 ,
C = max
1‚â§k‚â§N
e‚àítk‚à•Œ∏k‚à•2
2(eŒ±k+1 ‚àí1)2
(1 ‚àíe‚àítk)2
,
D = 1 ‚àíe‚àímax(Œ±N, tmax‚àítN)
etN ‚àí1
max{‚à•Œ∏N‚à•2
2 , ‚à•Œ∏(tmax)‚à•2
2} .
Then
sup
0‚â§t‚â§tmax
{ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))} ‚â§A max{B, C, D}
(9)
when tN < tmax and ‚à•Œ∏(tmax)‚à•2 < ‚àû, and
sup
0‚â§t‚â§tmax
{ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))} ‚â§A max{B, C}
(10)
when tN‚àí1 ‚â§tmax ‚â§tN for some N ‚â•1.
The above result provides a practical approach to verify whether the optimization error is dominated
by the interpolation error by checking (8). In other words, (8) is a sufficient condition for (7) and it can
be used as a stopping criterion at each grid point tk for any algorithm that computes a solution at tk.
We also remark that, using the above bounds, similar approximation-error bounds for ‚à•ÀúŒ∏(t) ‚àíŒ∏(t)‚à•2
can be derived as well (see Corollary S2 in the supplementary material).
Another important implication of Theorem 2 is that the established global approximation-error bound
provides a principled approach to choose the grid points so that for any œµ > 0 and tmax ‚àà(0, ‚àû] we
have
sup
0‚â§t‚â§tmax
{ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))} ‚â≤œµ
(11)
for the solution path generated by an algorithm, and at the same time it tries to maximize the speed
at which the algorithm explores the solution path from 0 to tmax. In particular, to ensure (11), we
propose to use the following step sizes
Œ±1 = min{Œ±max, ln(1 +
‚àöœµ
‚à•‚àáLn(0)‚à•2
)}
(12)
and
Œ±k+1 = min

ln(1 + c1(eŒ±1 ‚àí1)‚à•‚àáLn(0)‚à•2etk/2(1 ‚àíe‚àítk)
‚à•Œ∏k‚à•2
), Œ±max, 2Œ±k

,
(13)
5
Algorithm 1 A general path following algorithm.
Input: œµ > 0, C0 ‚â§1/4, c1 ‚â•1, c2 > 0, 0 < Œ±max ‚â§5‚àí1 and tmax ‚àà(0, ‚àû].
Output: grid points {tk}N
k=1 and an approximated solution path ÀúŒ∏(t).
1: Initialize: k = 1.
2: Compute Œ±1 using (12). Starting from 0, iteratively calculate Œ∏1 by minimizing ft1(Œ∏) until (8)
is satisfied for k = 1.
3: while (14) is not satisfied, do
4:
Compute Œ±k+1 using (13), update tk+1 = tk + Œ±k+1.
5:
Starting from Œ∏k, iteratively compute Œ∏k+1 by minimizing ftk+1(Œ∏) until (8) is satisfied.
6:
Update k = k + 1.
7: Interpolation: construct a solution path ÀúŒ∏(t) through linear interpolation of {Œ∏k}N
k=1 using (3).
and we terminate the algorithm at tN when
c2
 1 ‚àíe‚àímax(Œ±N, tmax‚àítN)
etN ‚àí1
‚â§œµ or tN ‚â•tmax ,
(14)
where c1 ‚â•1, c2 > 0, and 0 < Œ±max ‚â§1/5 are absolute constants. Note that the above step sizes
and termination criterion are chosen to ensure that the upper bounds in (9) and (10) are at most O(œµ).
Formally, we show that, given any œµ > 0, the above scheme achieves œµ-suboptimality (11) for ÀúŒ∏(t)
(up to a multiplicative constant).
Theorem 3. Suppose that Ln(Œ∏) is differentiable and convex. For any œµ > 0 and tmax ‚àà(0, ‚àû],
assume that either tmax < ‚àûor ‚à•Œ∏(tmax)‚à•2 < ‚àû. Then, using the step sizes and the termination
criterion specified above in (12), (13), and (14), any algorithm that produces iterates satisfying (8)
for any k ‚â•1 terminates after finite number of iterations, and when terminated, the solution path
ÀúŒ∏(t) satisfies
sup
0‚â§t‚â§tmax
{ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))} ‚â≤œµ .
(15)
Note that when œµ is small enough, we have that Œ±1 = ln (1 + ‚àöœµ/‚à•‚àáLn(0)‚à•2), which together with
(15) implies that
sup
0‚â§t‚â§tmax
{ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))} ‚â≤(eŒ±1 ‚àí1)2‚à•‚àáLn(0)‚à•2
2 .
As a result, the approximation error for the solution path is essentially controlled by the initial step
size Œ±1. Smaller Œ±1 leads to better approximation. However, smaller Œ±1 also leads to a slower
exploration of the solution path and a more stringent stopping criterion at individual grid points (c.f.
(8)). As such, the initial step size controls the trade-off between computational cost and accuracy.
Moreover, given Œ±1, we can see from (13) that how fast the algorithm can explore the solution path
depends largely on ‚à•Œ∏k‚à•2. In particular, if ‚à•Œ∏k‚à•2 is bounded (e.g., when Œ∏‚ãÜis finite), then the first
term in the min function of (13) increases as tk increases. Therefore, aggressive step sizes can be
taken in this case until it reaches O(1), which will likely lead to a fast exploration of the solution
path. On the other hand, if ‚à•Œ∏k‚à•2 grows quite quickly to infinity as k increases, then the first term in
the min function may go to zero as k ‚Üí‚àû(say, when ‚à•Œ∏k‚à•2/etk/2 ‚Üí‚àû). This means that the step
sizes need to decrease to zero eventually, leading to a slower exploration of the solution path.
We summarize the proposed scheme in Algorithm 1. Again, we emphasize that the above scheme
can be applied to any optimization algorithm. Later, we will empirically investigate its performance
using Newton method and gradient descent method.
Lastly, we establish an upper bound on the total number of grid points required for Algorithm 1 to
achieve œµ-suboptimality.
Theorem 4. Under the assumptions in Theorem 3, the total number of grid points required for
Algorithm 1 to achieve an œµ-suboptimality (15) is at most O(œµ‚àí1/2).
6
3
Extensions to nonconvex loss functions
So far we assume that the empirical loss is convex. Next, we consider a generalization, where we do
not assume convexity for Ln(Œ∏). Instead, we assume that there exists Œª(t) > 0 so that
ft(Œ∏) ‚àíft(Œ∏(t)) ‚â§
1
2Œª(t)‚à•‚àáft(Œ∏)‚à•2
2 ,
(16)
for any t ‚â•0 and Œ∏ ‚ààdom(Ln). In the literature, the condition (16) was often referred to as the
Polyak-≈Åojasiewicz (PL) inequality. Polyak [1964] proved the linear rate of convergence for gradient
descent method under this condition. Also see Karimi et al. [2016] for discussions of more recent
development. Note that if Ln(Œ∏) is convex, then (16) holds for Œª(t) = e‚àít since ft(Œ∏) is e‚àít-strongly
convex. In general, however, Ln(Œ∏) does not need to be convex for (16) to hold.
For some technical reasons, we need to consider slightly different interpolation scheme
ÀúŒ∏(t) = Œ∏k
for any t ‚àà[tk, tk+1], 0 ‚â§k ‚â§N ‚àí1;
ÀúŒ∏(t) = Œ∏N
for any tN < t ‚â§tmax if tN < tmax ,
(17)
where t0 = 0 and Œ∏0 = 0. As we can see that the approximated solution path is piecewise constant.
We use the following termination criterion at each tk
‚à•g1‚à•2 ‚â§min(C0(eŒ±1 ‚àí1)‚à•‚àáLn(0)‚à•2 , 1);
‚à•gk+1‚à•2 ‚â§min(C0
eŒ±k+1 ‚àí1
etk ‚àí1 ‚à•Œ∏k‚à•2 , 1), k ‚â•1 .
(18)
We also define a slightly new step size scheme as follows:
Œ±1 = min{Œ±max, ln(1 +
p
œµ min(Œª0, Œª1)
‚à•‚àáLn(0)‚à•2
)}
(19)
and
Œ±k+1 = min{Œ±max, 2Œ±k, ln(1 + c1(eŒ±1 ‚àí1)‚à•‚àáLn(0)‚à•2(etk ‚àí1)
p
min(Œªk, Œªk+1)
‚à•Œ∏k‚à•2
p
min(Œª0, Œª1)
)} ,
(20)
where Œªk = inftk‚â§t‚â§tk+1 Œª(t), k ‚â•0. Finally, the stopping criterion for the path-following algorithm
is set to be
c3
etN ‚àí1 ‚â§œµ or tN ‚â•tmax ,
(21)
where c3 > 0 is an absolute constant.
Moreover, we need to make some additional assumptions on the loss function.
Assumption (A1). Assume that Ln(Œ∏) is differentiable and infŒ∏ Ln(Œ∏) > ‚àí‚àû. Moreover, assume
that there exists a positive decreasing function g(¬∑) such that Œª(t) ‚â•g(t) > 0 for any 0 ‚â§t < ‚àû.
Theorem 5. Suppose that Assumption (A1) holds. For any œµ > 0 and tmax ‚àà(0, ‚àû], assume that
either tmax < ‚àûor sup0‚â§t‚â§tmax ‚à•Œ∏(t)‚à•2 < ‚àû. Then, using step sizes and termination criterion
specified in (19), (20), and (21), any algorithm that produces gradients satisfying (18) terminates
after finite number of iterations, and when terminated, the solution path ÀúŒ∏(t) defined in (17) satisfies
sup
0‚â§t‚â§tmax
{ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))} ‚â≤œµ .
(22)
The above result can be viewed as a generalization of Theorem 3, because when Ln(Œ∏) is convex, we
could set Œª(t) = exp(‚àít) so that both condition (16) and Assumption (A1) are satisfied.
4
Numerical studies
In this section, we study the operating characteristics of Newton method, gradient descent method and
a mixed method in ridge regression and ‚Ñì2-regularized logistic regression, using both the simulated
datasets and a real data example. Here the mixed method applies gradient descent method to minimize
7
ftk(Œ∏) at the beginning and then switch to Newton method when the number of gradient steps nk
exceeds min{n, p}. To illustrate the advantages of the proposed grid point selection scheme and
stopping criterion, we use a standard implementation of a typical path-following algorithm as a
baseline. In particular, the grid points are equally spaced (on a log-scale) for both examples. For
‚Ñì2 logistic regression, Newton method is used as the basis algorithm with a fixed stopping criterion
‚à•‚àáftk(Œ∏k)‚à•2 < 10‚àí5, while for ridge regression the exact solution can be computed. We refer to
this method as the baseline method.
We also compare the proposed methods against the method of Ndiaye et al. [2019]. In particular,
we compare our grid points selection scheme (13) with the so-called adaptive unilateral scheme
proposed in Ndiaye et al. [2019], where Newton method is used as the basis algorithm at each
grid point. Different from the aforementioned methods, adaptive unilateral scheme constructs
piecewise constant solution path ÀúŒ∏(Œª) over chosen grid points to approximate the solution path
Œ∏(Œª) = arg min Œ∏‚ààRp PŒª(Œ∏), where PŒª(Œ∏) = Ln(Œ∏)+(Œª/2)¬∑‚à•Œ∏‚à•2
2, such that for all Œª ‚àà(Œªmin, Œªmax)
PŒª(ÀúŒ∏(Œª)) ‚àíPŒª(Œ∏(Œª)) < œµ .
(23)
We call this Ndiaye‚Äôs method throughout this section.
All methods considered in the numerical studies are implemented in R using Rcpp package [Eddel-
buettel et al., 2011, Eddelbuettel, 2013].
Example 1: Ridge regression. This example considers ridge regression over a simulated dataset.
In this case, the empirical loss function is Ln(Œ∏) = ‚à•Y ‚àíXŒ∏‚à•2
2/(2n), where X ‚ààRn√óp and
Y ‚ààRn denote the design matrix and the response vector. In the simulation, the data (X, Y )
are generated from the usual linear regression model Y = XŒ∏‚ãÜ+ Àúœµ, where Àúœµ ‚àºN(0, In√ón),
Œ∏‚ãÜ= (1/‚àöp, . . . , 1/‚àöp)‚ä§, and rows of X are IID samples from N(0, Ip√óp). Throughout this
example, we consider n = 1000 and p = 500, 10000,
Example 2: ‚Ñì2-regularized logistic regression.
This example considers ‚Ñì2-regularized logistic
regression over a simulated dataset. Let X ‚ààRn and Y ‚àà{+1, ‚àí1}n denote the design matrix
and the binary response vector. The empirical loss function for logistic regression is Ln(Œ∏) =
n‚àí1 Pn
i=1 log(1 + exp(‚àíYiX‚ä§
i Œ∏)). We simulate the data (X, Y ) from a linear discriminant analysis
(LDA) model. More specifically, we sample the components of Y independently from a Bernoulli
distribution with P(Yi = +1) = 1/2; i = 1, 2, . . . , n. Conditioned on Yi, Xi‚Äôs are then independently
drawn from N(Yi¬µ, œÉ2Ip√óp), where ¬µ ‚ààRp and œÉ2 > 0. Note that ¬µ and œÉ2 controls the Bayes
risk, which is Œ¶(‚àí‚à•¬µ‚à•2/œÉ) under the 0/1 loss, where Œ¶(¬∑) is the cumulative distribution function
of a standard normal random variable. Here we choose ¬µ = (1/‚àöp, . . . , 1/‚àöp) and œÉ2 = 1 so that
the Bayes risk is Œ¶(‚àí1), which is approximately 15%. Similar to Example 1, two different problem
dimension are considered: n = 1000 and p = 500, 10000.
Example 3: Real data example. This example fits an ‚Ñì2-regularized logistic regression using the
a9a dataset from LIBSVM [Chang and Lin, 2011]. This dataset provides information about whether
an adult‚Äôs annual income surpasses $50, 000, along with some specific characteristics of the given
adult. It contains a total of 32561 examples with 123 features and 1 binary response variable.
For all three examples, we use the global approximation error sup0‚â§t‚â§tmax{ft(ÀúŒ∏(t)) ‚àíft(Œ∏(t))} as
an evaluation metric, where ÀúŒ∏(t) is the linear interpolation of the iterates Œ∏k generated by each method.
Throughout, we let tmax = 10. Since this metric can not be calculated exactly, we approximate
the global approximation error by max1‚â§i‚â§100{fsi(ÀúŒ∏(si)) ‚àífsi(Œ∏(si))}, where s1, . . . , s100 are
uniformly sampled from (0, tmax). Here we compute the solutions Œ∏(si) using the CVX solver [Grant
and Boyd, 2014, 2008] for ‚Ñì2-regularized logistic regression, while calculating Œ∏(si) explicitly for
ridge regression.
We compare Newton method, gradient descent method and their mixed version against the baseline
method and Ndiaye‚Äôs method. To make a fair comparison with Ndiaye‚Äôs method, we set Œªmin =
1/(e10 ‚àí1) to match tmax = 10 in log scale and choose Œªmax = 100.
Figure 1 presents runtime versus global approximation error for Example 1 and 2 based on 100
replications, as we vary œµ in (12) for our proposed method and (23) for Ndiaye‚Äôs method. For the
baseline method, we vary the spacing between consecutive grid points instead. Clearly the proposed
Newton method outperforms the baseline method and Ndiaye‚Äôs method in both low dimension and
high dimension scenarios, which empirically confirms that the proposed grid point selection scheme
8
(c.f. (13)) is more efficient than both the standard equally-spaced grid point scheme and the adaptive
unilateral scheme proposed by Ndiaye et al. [2019]. Moreover, gradient descent method performs
similarly compared with the mixed method in both setups of Example 1, which suggests that switching
does not happen much in those cases. However, in both problem dimensions of Example 2, we do
see a difference between the mixed method and gradient descent, which is due to the fact that the
switching to the Newton update does happen and it can speed up the computation by design. Lastly,
as one would expect, Newton method is more efficient than both the gradient descent method and the
mixed method when the problem dimension is small, but less so when problem dimension is large.
Figure 2 shows the runtime versus global approximation error tradeoff for our real data example, as
we vary œµ for our proposed methods and Ndiaye‚Äôs method, and change grid point spacing for the
baseline method. Again, the Newton method is more efficient than both the baseline method and the
Ndiaye‚Äôs method, which demonstrates the advantage of our proposed grid point selection scheme
over the standard equally-spaced grid point scheme and the scheme by Ndiaye et al. [2019].
We also plot the number of iterations at each grid point for Newton method and gradient descent
method in Figure S1-S3 of the supplementary material. Interestingly, for ridge regression (c.f.,
Figure S1), the number of iterations by gradient method first increases and then stays flat as tk
grows. Newton method, however, only takes one iteration at each grid point. Moreover, the level of
approximation (i.e., œµ) seems to have no impact on the number of iterations at each grid point, which
is highly desirable. For the ‚Ñì2-regularized logistic regression (c.f., Figure S2 and S3), the number of
iterations needed increases as tk increases for gradient descent method, whereas the Newton method
always requires just a constant number of iterations at each tk. Again, the level of approximation
(i.e., œµ) does not seem to influence the number of iterations much.
‚àí8
‚àí6
‚àí4
‚àí2
0.10
1.00
10.00
Runtime (in seconds)
Suboptimality
Baseline method
Gradient descent
Mixed method
Ndiaye
Newton
p=500
‚àí6
‚àí4
‚àí2
10.00
100.00
1 000.00
Runtime (in seconds)
Suboptimality
Baseline method
Gradient descent
Mixed method
Ndiaye
Newton
p=10000
‚àí8
‚àí6
‚àí4
1.00
10.00
100.00
Runtime (in seconds)
Suboptimality
Baseline method
Gradient descent
Mixed method
Ndiaye
Newton
p=500
‚àí6
‚àí4
‚àí2
1.00
10.00
100.00
1 000.00
10 000.00
Runtime (in seconds)
Suboptimality
Baseline method
Gradient descent
Mixed method
Ndiaye
Newton
p=10000
Figure 1: Runtime v.s. suboptimality for the Newton method, the gradient descent method, the mixed
method, the baseline method and Ndiaye‚Äôs method under two problem dimensions when applied to
ridge regression (upper panels) and ‚Ñì2-regularized logistic regression (lower panels). The vertical and
horizontal lines at each point represent the standard errors of suboptimality and runtime.
5
Discussion
In this article, we proposed a novel grid point selection scheme and stopping criterion for any general
path-following algorithms. A simple solution path was constructed through linear interpolation
9
‚àí10
‚àí8
‚àí6
‚àí4
‚àí2
10.00
100.00
1 000.00
Runtime (in seconds)
Suboptimality
Baseline method
Gradient descent
Mixed method
Ndiaye
Newton
Figure 2: Runtime v.s. suboptimality for the Newton method, the gradient descent method, the mixed
method, the baseline method and Ndiaye‚Äôs method when applied to ‚Ñì2-regularized logistic regression
over the a9a real dataset from LIBSVM [Chang and Lin, 2011].
and theoretical approximation-error bound was derived. A generalization to a class of nonconvex
empirical loss was also considered. We have not touched on the overall computational complexity
analysis for commonly used optimization algorithms. Based on some preliminary analysis, we
conjecture that for gradient descent method and Newton method, the number of iterations required
are O(œµ‚àí1 ln(œµ‚àí1)) and O(œµ‚àí1/2), respectively. Moreover, lower bound results are also worth
investigating, that is, what are the minimum number of gradient or Newton steps needed to achieve an
œµ-suboptimality for the entire solution path? Moreover, our current analysis can be readily extended
to general differentiable regularization functions. A more challenging future direction is to investigate
the case where the empirical loss function or the regularizer are not differentiable.
Acknowledgments and Disclosure of Funding
The authors are supported in part by the US National Science Foundation (NSF) under grant DMS-17-
12580, DMS-17-21445 and DMS-20-15490. The authors thank the reviewers for valuable comments
and suggestions that improved the article.
References
T. B. Arnold and R. J. Tibshirani. Efficient implementations of the generalized lasso dual path algorithm. Journal
of Computational and Graphical Statistics, 25(1):1‚Äì27, 2016. doi: 10.1080/10618600.2015.1008638.
R. Bao, B. Gu, and H. Huang. Efficient approximate solution path algorithm for order weight l_1-norm with
accuracy guarantee. In 2019 IEEE International Conference on Data Mining (ICDM), pages 958‚Äì963. IEEE,
2019.
H. H. Bauschke, P. L. Combettes, et al. Convex analysis and monotone operator theory in Hilbert spaces,
volume 408. Springer, 2011.
C.-C. Chang and C.-J. Lin. Libsvm: a library for support vector machines. ACM transactions on intelligent
systems and technology (TIST), 2(3):1‚Äì27, 2011.
D. Eddelbuettel. Seamless R and C++ integration with Rcpp. Springer, 2013.
D. Eddelbuettel, R. Fran√ßois, J. Allaire, K. Ushey, Q. Kou, N. Russel, J. Chambers, and D. Bates. Rcpp:
Seamless r and c++ integration. Journal of Statistical Software, 40(8):1‚Äì18, 2011.
B. Efron, T. Hastie, I. Johnstone, and R. Tishirani. Least angle regression. The Annals of Statistics, 32(2):407 ‚Äì
499, 2004.
J. Friedman, T. Hastie, H. Hofling, and R. Tibshirani. Pathwise coordinate optimization. The Annals of Applied
Statistics, 1(2):302‚Äì332, 2007.
P. Garrigues and L. Ghaoui. An homotopy algorithm for the lasso with online observations. Advances in neural
information processing systems, 21, 2008.
10
J. Giesen, M. Jaggi, and S. Laue. Approximating parameterized convex optimization problems. In European
Symposium on Algorithms, pages 524‚Äì535. Springer, 2010.
J. Giesen, J. M√ºller, S. Laue, and S. Swiercy. Approximating concavely parameterized optimization problems.
In Advances in neural information processing systems, pages 2105‚Äì2113, 2012.
J. Giesen, S. Laue, and P. Wieschollek. Robust and efficient kernel hyperparameter paths with guarantees. In
International Conference on Machine Learning, pages 1296‚Äì1304, 2014.
M. Grant and S. Boyd. Cvx: Matlab software for disciplined convex programming, version 2.1, 2014.
M. C. Grant and S. P. Boyd. Graph implementations for nonsmooth convex programs. In Recent advances in
learning and control, pages 95‚Äì110. Springer, 2008.
T. Hastie, S. Rosset, R. Tishirani, and J. Zhu. The entire regularization path for the support vector machine.
Journal of Machine Learning Research, 5:1391 ‚Äì 1415, 2004.
T. Hastie, J. Taylor, R. Tibshirani, G. Walther, et al. Forward stagewise regression and the monotone lasso.
Electronic Journal of Statistics, 1:1‚Äì29, 2007.
H. Hoefling. A path algorithm for the fused lasso signal approximator. Journal of Computational and Graphical
Statistics, 19(4):984‚Äì1006, 2010. doi: 10.1198/jcgs.2010.09208.
H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods under the
polyak-≈Çojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery
in Databases, pages 795‚Äì811. Springer, 2016.
E. Ndiaye. Safe optimization algorithms for variable selection and hyperparameter tuning. PhD thesis, Universit√©
Paris-Saclay (ComUE), 2018.
E. Ndiaye, T. Le, O. Fercoq, J. Salmon, and I. Takeuchi. Safe grid search with optimal complexity. In
International Conference on Machine Learning, pages 4771‚Äì4780, 2019.
Y. E. Nesterov and A. S. Nemirovskii. Interior Point Polynomial Methods in Convex Programming: Theory and
Algorithms. SIAM Publications, 1993.
M. Osborne. An effective method for computing regression quantiles. IMA Journal of Numerical Analysis, 12:
151 ‚Äì 166, 1992.
M. Osborne, B. Presnell, and B. Turlach. A new approach to variable selection in least squares problems. IMA
Journal of Numerical Analysis, 20(3):389 ‚Äì 403, 2000.
B. T. Polyak. Gradient methods for solving equations and inequalities. USSR Computational Mathematics and
Mathematical Physics, 4(6):17‚Äì32, 1964.
X. Qu, D. Li, X. Zhao, and B. Gu. Gaga: Deciphering age-path of generalized self-paced regularizer. Advances
in Neural Information Processing Systems, 35:32025‚Äì32038, 2022.
S. Rosset. Following curved regularized optimization solution paths. In Advances in Neural Information
Processing Systems, 2004.
S. Rosset and J. Zhu. Piecewise linear regularized solution paths. Ann. Statist., 35(3):1012‚Äì1030, 2007. doi:
10.1214/009053606000001370.
T. Sun and Q. Tran-Dinh. Generalized self-concordant functions: a recipe for newton-type methods. Mathemati-
cal Programming, pages 1‚Äì69, 2017.
R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. The Annals of Statistics, pages
1335‚Äì1371, 2011.
Y. Zhu and R. Liu. An algorithmic view of ‚Ñì2-regularization and some path-following algorithms. The Journal
of Machine Learning Research, 22(1):6123‚Äì6184, 2021.
11
