Natural Actor-Critic for Robust Reinforcement
Learning with Function Approximation
Ruida Zhou∗
Texas A&M University
ruida@tamu.edu
Tao Liu∗
Texas A&M University
tliu@tamu.edu
Min Cheng
Texas A&M University
minrara0404@tamu.edu
Dileep Kalathil
Texas A&M University
dileep.kalathil@tamu.edu
P. R. Kumar
Texas A&M University
prk@tamu.edu
Chao Tian
Texas A&M University
chao.tian@tamu.edu
Abstract
We study robust reinforcement learning (RL) with the goal of determining a well-
performing policy that is robust against model mismatch between the training sim-
ulator and the testing environment. Previous policy-based robust RL algorithms
mainly focus on the tabular setting under uncertainty sets that facilitate robust pol-
icy evaluation, but are no longer tractable when the number of states scales up. To
this end, we propose two novel uncertainty set formulations, one based on double
sampling and the other on an integral probability metric. Both make large-scale
robust RL tractable even when one only has access to a simulator. We propose a
robust natural actor-critic (RNAC) approach that incorporates the new uncertainty
sets and employs function approximation. We provide finite-time convergence
guarantees for the proposed RNAC algorithm to the optimal robust policy within
the function approximation error. Finally, we demonstrate the robust performance
of the policy learned by our proposed RNAC approach in multiple MuJoCo envi-
ronments and a real-world TurtleBot navigation task.
1
Introduction
Training a reinforcement learning (RL) algorithm directly on a real-world system is expensive and
potentially risky due to the large number of data samples required to learn a satisfactory policy. To
overcome this issue, RL algorithms are typically trained on a simulator. However, in most real-
world applications, the nominal model used in the simulator model may not faithfully represent
the real-world system due to various factors, such as approximation errors in modeling or varia-
tions in real-world parameters over time. For example, the mass, friction, sensor/actuator noise,
and floor terrain in a mobile robot simulator may differ from those in the real world. This mis-
match, called simulation-to-reality-gap, can significantly degrade the performance of standard RL
algorithms when deployed on real-world systems [36, 44, 55, 51]. The framework of robust Markov
decision process (RMDP) [19, 40] is used to model this setting where the testing environment is
uncertain and comes from an uncertainty set around the nominal model. The optimal robust policy
is defined as the one which achieves the optimal worst-case performance over all possible models
in the uncertainty set. The goal of robust reinforcement learning is to learn such an optimal robust
policy using only the data sampled from the simulator (nominal) model.
The RMDP planning problem has been well studied in the tabular setting [63, 62, 35, 46, 16]. Also,
many works have developed model-based robust RL algorithms in the tabular setting [66, 69, 42,
∗The first two authors contributed equally.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).
64, 49], focusing on sample complexity. Many robust Q-learning algorithms [45, 33, 39, 59, 30]
and policy gradient methods for robust RL [61, 27, 29, 58, 17] have been developed for the tabular
setting . Different from all these works, the main goal of this paper is to develop a computationally
tractable robust RL algorithm with provable convergence guarantees for RMDPs with large state
spaces, using linear and nonlinear function approximation for robust value and policy.
One of the main challenges of robust RL with a large state space is the design of an effective un-
certainty set that is amenable to computationally tractable learning with function approximation.
Robust RL algorithms, both in their implementation and technical analysis, require robust Bell-
man operator evaluations [19, 40] which involve an inner optimization problem over the uncertainty
set. Performing this inner optimization problem and/or getting an unbiased estimate of the robust
Bellman operator evaluation using only the samples from the nominal model can be intractable for
commonly considered uncertainty sets when the state space is large. For example, for f-divergence-
based uncertainty sets [66, 69, 64, 49, 33, 30], the robust Bellman operator estimate requires solving
for a dual variable associated with each state, which is prohibitive for large state spaces. For R-
contamination [61], the estimate requires calculating the minimum of the value function over state
space, which is impractical for large state spaces. It is also infeasible for ℓp norm-based uncertainty
sets [27], where the estimates require calculating the median, mean, or average peak of the value
function depending on the choice of p. Robust RL with function approximation has been explored in
a few works [53, 41, 45, 43, 60, 6]. However, these works explicitly or implicitly assume an oracle
that approximately computes the robust Bellman operator estimate, and the uncertainty set design
that facilitates computation and learning is largely ignored. We overcome this challenge by intro-
ducing two novel uncertainty set formulations, one based on double sampling (DS) and the other
on an integral probability metric (IPM). Both are compatible with large-scale robust MDP, with the
robust Bellman operators being amenable to unbiased estimation from the data sampled from the
nominal model, enabling effective practical robust RL algorithms.
Policy-based RL algorithms [47, 31, 18, 15], which optimize the policy directly, have been extremely
successful in learning policies for continuous control tasks. Value-based RL approaches, such as Q-
learning, cannot be directly applied to continuous action problems. Moreover, recent advances in
policy-based approaches can establish finite time convergence guarantees and also offer insights into
the practical implementation [3, 37, 4, 8]. However, most of the existing works on robust RL with
function approximation use value-based approaches [53, 41, 45, 43, 60, 34] that are not scalable to
continuous control problems. On the other hand, the existing works that use policy-based methods
for robust RL with convergence guarantees are limited to the tabular setting [61, 27, 29, 58, 17]. We
close this important gap in the literature by developing a novel robust natural actor-critic (RNAC)
approach that leverages our newly designed uncertainty sets for scalable learning.
Summary of Contributions: (i). We propose two novel uncertainty sets, one using double sampling
(Section 3.1) and the other an integral probability metric (Section 3.2), which are both compatible
for robust RL with large state space and function approximation. Though robust Bellman operators
require the unknown worst-case models, we provide unbiased empirical robust Bellman operators
that are computationally easy to utilize and only based on samples from the nominal model;
(ii). We propose a novel RNAC algorithm (Section 4) which to the best of our knowledge is the
first policy-based approach for robust RL under function approximation, with provable convergence
guarantees. We consider both linear and general function approximation for theoretical study, with
the latter relegated to Appendix E due to space constraints. Under linear function approximation,
the RNAC with a robust critic performing “robust linear-TD” (Section 5) and a robust natural actor
performing “robust Q-NPG” (Section 6) is proved to converge to the optimal robust policy within the
function approximation error. Specifically, ˜O(1/ε2) sample complexity can be achieved, or ˜O(1/ε3)
for policy update with a constant step size. For the robust linear-TD, we study the contraction
behavior of the projected robust Bellman operator for RMDPs with the proposed uncertainty sets
and the well-known f-divergence uncertainty sets, which we believe is of independent interest;
(iii). We implement the proposed RNAC in multiple MuJoCo environments (Hopper-v3, Walker2d-
v3, and HalfCheetah-v3), and demonstrate that RNAC with the proposed uncertainty sets results in
robust behavior while canonical policy-based approaches suffer significant performance degrada-
tion. We also test RNAC on TurtleBot [5], a real-world mobile robot, performing a navigation task.
We show that the TurtleBot with RNAC successfully reaches its destination while canonical non-
2
robust approaches fail under adversarial perturbations. A video of the demonstration on TurtleBot
is available at [Video Link] and the RNAC code is provided in the supplementary material.
Due to the page limit, a detailed literature review and comparison of our work with the existing
robust RL algorithms are deferred to Appendix G.
2
Preliminaries
Notations: For any set X, denote by |X| its cardinality, by ∆X a (|X| −1)-dimensional probability
simplex, and by Unif(X) a uniform distribution over X. Let [m] := {1, . . . , m}.
A Markov decision process (MDP) is represented by a tuple (S, A, κ, r, γ), where S is the
state space, A is the action space, κ = (κ0, κ1, . . .) is a possibly non-stationary transition ker-
nel sequence with κt : S × A →∆S, r : S × A →[0, 1] is the reward function, and
γ ∈(0, 1) is the discount factor.
For a stationary policy π : S →∆A, its value func-
tion is V π
κ (s) := Eκ,π [P∞
t=0 γtr(st, at)|s0 = s] , where the expectation is taken w.r.t. the tra-
jectory (s0, a0, s1, a1, . . .) with at|st ∼πst and st+1|(st, at) ∼κt,st,at.
We can similarly
define the state visitation distribution under initial distribution ρ ∈∆S, dπ,κ
ρ
(s) := (1 −
γ)Eκ,π [P∞
t=0 γt1(st = s)|s0 ∼ρ]; the state-action value function (Q function), Qπ
κ(s, a) :=
Eκ,π [P∞
t=0 γtr(st, at)|s0 = s, a0 = a] , and the advantage function Aπ
κ(s, a) := Qπ
κ(s, a)−V π
κ (s).
Robust MDP: A RMDP is represented by a tuple (S, A, P, r, γ), where P is a set of transition
kernels known as the uncertainty set that captures the perturbations around the nominal stationary
kernel p◦: S × A →∆S, and its robust value function is defined as the corresponding worst case:
V π
P (s) :=
inf
κ∈N
t≥0 P V π
κ (s).
(1)
We will assume the following key (s, a)-rectangularity condition that is commonly assumed to fa-
cilitate dynamic programming ever since the introduction of RMDPs [19, 40]:
Definition 1. P is (s, a)-rectangular, if P = N
s,a Ps,a, for some Ps,a ⊆∆S.
The corresponding robust Bellman operator T π
P : RS →RS is
(T π
P V )(s) = Ea∼π(·|s)
h
r(s, a) + γ
inf
p∈Ps,a p⊤V
i
,
∀s ∈S, V ∈RS,
(2)
and the Bellman equation for RMDPs is V = T π
P V , where V = V π is its unique solution from the
Banach fixed-point theorem. Typically, Ps,a is taken as a ball {ν ⊆∆S : d(ν, p◦
s,a) ≤δ} around
a nominal model p◦of the training environment, where d(·, ·) is some divergence measure between
probability distributions, and δ > 0 controls the level of robustness.
There is a stationary optimal policy π∗that uniformly maximizes the robust value function, i.e.,
V π∗
P (s) = sup{V π
P (s): history dependent π}, ∀s [19, 40]. Thus, without loss of generality, we only
need to optimize within stationary policies. Moreover, for any stationary policy π there exists a
stationary worst-case kernel κπ with V π
κπ(s) = V π
P (s), ∀s. We can define the robust Q-function
and robust advantage function as Qπ
P(s, a) := Qπ
κπ(s, a) and Aπ
P(s, a) := Aπ
κπ(s, a), respectively.
When the uncertainty set is clear from the context, we will omit the subscript P in V π
P , Qπ
P, and Aπ
P.
To summarize, the motivation in the robust RL framework is to learn the optimal robust policy by
only training on a simulator implementing an (unknown) nominal model p◦[53, 45, 41], with the
robust RL algorithms only having access to data generated from p◦, and not from any other model
in the uncertainty set P.
3
Uncertainty Sets for Large State Spaces
Evaluating the robust Bellman operator T π
P requires solving the optimization infp∈Ps,a p⊤V , which
is challenging for arbitrary uncertainty sets. Moreover, since we only have access to data generated
by the nominal model p◦, estimating T π
P is not straightforward. Previously studied uncertainty sets,
such as R-contamination, f-divergence, and ℓp norm, are intractable for RMDPs with large state
spaces for these reasons (see Appendix B for detailed explanation). We therefore design two uncer-
tainty sets for RMDPs where the robust Bellman operator has a tractable form and can be unbiasedly
3
estimated by the data sampled from the nominal model, thus making effective learning possible. We
will also use function approximation to tractably parameterize policy and value functions.
3.1
Double-Sampling (DS) Uncertainty Set
The central difficulty in employing the robust Bellman operator (2) is how to evaluate
infp∈Ps,a p⊤V .
Our first method is based on the following key idea for drawing samples
that produce an unbiased estimate for it.
Let {s′
1, s′
2, . . . , s′
m} be m samples drawn i.i.d. ac-
cording to the nominal model p◦
s,a.
Then, for any given divergence measure d(·, ·) and ra-
dius δ
>
0, there exists an uncertainty set P
=
⊗s,aPs,a such that infp∈Ps,a p⊤V
=
Es′
1:m
i.i.d.
∼p◦s,a
h
infα∈∆[m]:d(α,Unif([m]))≤δ
Pm
i=1 αiV (s′
i)
i
. (See Appendix B.1 for a brief explana-
tion.) We therefore define an empirical robust Bellman operator ˆT π
P corresponding to (2) by
( ˆT π
P V )(s, a, s′
1:m) := r(s, a) + γ
inf
α∈∆[m]:d(α,Unif([m]))≤δ
m
X
i=1
αiV (s′
i).
(3)
Since (T π
P V )(s) = Ea∼π(·|s), s′
1:m
i.i.d.
∼p◦s,a
h
( ˆT π
P V )(s, a, s′
1:m)
i
, ˆT π
P gives an unbiased estimate,
which is one key property we use in our robust RL algorithm. We refer to this as “double sam-
pling” since Pm
i=1 αiV (s′
i) is the expected value when one further chooses one sample s′ from
{s′
1, s′
2, . . . , s′
m} according to the distribution α on [m] and evaluates V (s′). Above, α is a per-
turbation of Unif([m]) and when δ = 0, α = Unif([m]) and s′ ∼p◦
s,a. The uncertainty set P
corresponding to the double sampling is implicitly defined by specifying the choices of m, d(·, ·),
and δ. Its key advantage is that samples can only be drawn according to the nominal model p◦.
Double sampling requires sampling multiple next states for a given state-action pair, which can be
implemented if the simulator is allowed to set to any state. (All MuJoCo environments [56] support
DS.) Since the calculation of α is within ∆[m], the empirical robust Bellman operator is tractable
for moderate values of m. We use m = 2 in experiments for training efficiency, where for almost
all divergences d(·, ·) we can explicitly write
( ˆT π
P V )(s, a, s′
1:2) = r(s, a) + γ (0.5(V (s′
1) + V (s′
2)) −δ|V (s′
1) −V (s′
2)|) .
(4)
Bellman completeness (value function class closed under the Bellman operator) is a key property for
efficient reinforcement learning, whereas RMDP with canonical uncertainty sets may violate it. We
show in Appendix B.1 that for RMDP with DS uncertainty sets, the linear function approximation
class satisfies Bellman completeness if the nominal model is a linear MDP [22].
3.2
Integral Probability Metric (IPM) Uncertainty Set
Given some function class F ⊂RS including the zero function, the integral probability metric
(IPM) is defined by dF(p, q) := supf∈F{p⊤f −q⊤f} ≥0 [38]. Many metrics such as Kantorovich
metric, total variation, etc., are special cases of IPM under different function classes [38].
The robust Bellman operator T π
P V (2) requires solving the optimization infq∈Ps,a q⊤V . For an
IPM-based uncertainty set P with Ps,a = {q : dF(q, p◦
s,a) ≤δ}, we relax the domain q ∈∆S to
P
s q(s) = 1 as done in [28, 27], which incurs no relaxation error for small δ if mins′ p◦
s,a(s′) > 0.
One should choose F so that it properly encodes information of the MDP and its value functions.
We start by considering linear function approximation. Denote by Ψ ∈RS×d the feature matrix
with rows ψ⊤(s), ∀s ∈S. The value function approximation is Vw = Ψw ∈RS. A good choice of
feature vectors encodes information about the state and transition. For example, vectors ψ(s), ψ(s′)
should be “close” when states s, s′ are “similar” and V π(s), V π(s′) have small differences. We
propose the following function class (with ℓ2 as the preferred norm but any other can be used):
F := {s 7→ψ(s)⊤ξ : ξ ∈Rd, ∥ξ∥≤1}.
(5)
Without loss of generality, assume Ψ has full column rank since d ≪|S|, and let the first coordinate
of ψ(s) be 1 for any s, which corresponds to the bias term of the linear regressor.
Proposition 1. For the IPM with F in (5), we have infq∈Ps,a q⊤Vw = (p◦
s,a)⊤Vw −δ∥w2:d∥.
4
Algorithm 1: Robust Natural Actor-Critic
Input: T, η0:T −1, K, N
Initialize: θ0 for policy parameterization and winit for value function approximation
for t = 0, 1, . . . , T −1 do
Robust critic updates wt;
//E.g., wt = RLTD(πθt, K) Algorithm 2
Robust natural actor updates θt+1;//E.g., θt+1 = RQNPG(θt, ηt, wt, N) Algorithm 3
We can thus design the empirical robust Bellman operator, which is an unbiased estimator of T π
P
with sample s′ drawn from the nominal model:
( ˆT π
P Vw)(s, a, s′) := r(s, a) + γVw(s′) −γδ∥w2:d∥.
(6)
Guided by the last regularization term of the empirical robust Bellman operator (6), when consider-
ing value function approximation by neural networks we add a similar negative regularization term
for all the neural network parameters except for the bias parameter in the last layer.
4
Robust Natural Actor-Critic
We propose a robust natural actor-critic (RNAC) approach in Algorithm 1 for the robust RL problem.
As its name suggests, there are two components – a robust critic and a robust actor, which update
alternately for T steps. At each step t, there is a policy πt determined by parameter θt. The robust
critic updates the value function approximation parameter wt based on on-policy trajectory data
sampled by executing πt on the nominal model with length K. The robust actor then updates the
policy with step size ηt, and the critic returns wt by on-policy trajectory data with length N. In
practice, a batch of on-policy data can be sampled and used for both critic and actor updates.
We now give the main (informal) convergence results for our RNAC algorithm with linear function
approximation and DS or IPM uncertainty sets, where the robust critic performs robust linear-TD
and the robust natural actor performs robust-QNPG (as the comments in Algorithm 1). The formal
statements, proofs, and generalization to general function approximation are given in Appendix E.
Theorem 1 (Informal linear convergence of RNAC). Under linear function approximation, RNAC
in Algorithm 1 with DS or IPM uncertainty sets using an RLTD robust critic update and an RQNPG
robust natural actor update, with appropriate geometrically increasing step sizes ηt, achieves
E[V π∗(ρ) −V πT (ρ)] = O(e−T ) + O(ϵstat) + O(ϵbias) and an ˜O(1/ε2) sample complexity.
The optimality gap is bounded in this theorem via three terms, where the first term, related to the
number of time steps T, is the optimization rate (linear convergence since O(e−T )), the second term
ϵstat = ˜O(
1
√
N +
1
√
K ) is a statistical error that depends on the number of samples K, N in the robust
critic and robust actor updates, and the last term ϵbias is the approximation error due to the limited
representation power of value function approximation and the parameterized policy class. Omitting
the approximation error ϵbias, the sample complexities for achieving ε robust optimal value are
˜O(1/ε2), which achieves the optimal sample complexity in tabular setting [64]. However, RNAC
with geometrically increasing step sizes induces a larger multiplicative constant factor (not shown
in big-O notation) and does not generalize well to general function approximation. We then analyze
RNAC with a constant step size.
Theorem 2 (Informal sublinear convergence of RNAC). RNAC under the same specification as
in Theorem 1 but with constant step size ηt = η has E[V π∗(ρ) −1
T
PT −1
t=0 V πt(ρ)] = O( 1
T ) +
O(ϵstat) + O(ϵbias), implying an ˜O(1/ε3) sample complexity.
Although the theorem shows a slower optimization rate of RNAC with constant step size, this non-
increasing step size is preferred in practice. Moreover, the analysis can be generalized to a general
policy class with optimization rate O(1/
√
T), and an ˜O(1/ε4) sample complexity.
5
Algorithm 2: Robust Linear Temporal Difference (RLTD)
Input: π, K
Initialize: w0, s0
for k = 0, 1, . . . , K −1 do
Sample ak ∼π(·|sk), yk+1 according to p◦
sk,ak, and sk+1 from yk+1
Update wk+1 = wk + αkψ(sk)
h
( ˆT π
P Vwk)(sk, ak, yk+1) −ψ(sk)⊤wk
i
// For DS: yk+1 = s′
1:m
i.i.d.
∼p◦
sk,ak, sk+1 ∼Unif(yk+1) and ˆT π
P in (3)
// For IPM: yk+1 = sk+1 ∼p◦
sk,ak and ˆT π
P in (6)
Return: wK
5
Robust Critic
The robust critic estimates the robust value function with access to samples from the nominal model.
One may note that in many previous actor-critic analyses for canonical RL [61, 29, 9], the critic
learns the Q function, while realistic implementations in on-policy algorithms (e.g., proximal policy
optimization (PPO)) treat the V function as the target of the critic for training efficiency. We consider
a robust critic that learns the robust V function to align with such realistic implementations.
We present the robust linear temporal difference (RLTD) Algorithm 2, which is similar to the
canonical linear-TD algorithm, but with an empirical robust Bellman operator. It iteratively per-
forms the sampling and updating procedures. The sampling procedure differs for the uncertainty
set by double sampling and IPM as shown in the comments in Algorithm 2. Using the samples, the
parameter for the linear value function approximation Vwk = Ψwk is updated with step size αk.
The following assumption is common [10, 9, 29]:
Assumption 1 (Geometric mixing). For any policy π, the Markov chain {sk} induced by applying
π in the nominal model p◦is geometrically ergodic with a unique stationary distribution νπ.
The update procedure essentially minimizes the Mean Square Projected Robust Bellman Error
MSPRBEπ(w) = ∥Ππ(T π
P Vw) −Vw∥2
νπ, where ∥V ∥νπ =
pP
s νπ(s)V (s)2 is the νπ weighted
norm, and Ππ = Ψ(Ψ⊤DπΨ)−1Ψ⊤Dπ is the weighted projection matrix with Dπ = diag(νπ) ∈
RS×S. The minimizer of MSPRBEπ(w), denoted by wπ, is the unique solution of the projected
robust Bellman equation Vw = ΠπT π
P Vw, which is equivalent to 0 = Ψ⊤Dπ(T π
P Ψw −Ψw). RLTD
is thus a stochastic approximation algorithm since the empirical operator ˆT π
P ((3) or (6)) is unbiased
with E(s,a,y′)∼νπ◦π◦p◦
h
ψ(s)
h
( ˆT π
P Vw)(s, a, y′) −ψ(s)⊤w
ii
= Ψ⊤Dπ(T π
P Vw −Vw).
To ensure that RLTD converges to the optimal linear approximation Vwπ, it is crucial that the pro-
jected robust Bellman operator ΠπT π
P be a contraction map with some β < 1.
Definition 2. ΠπT π
P is a β-contraction w.r.t. ∥· ∥νπ if ∥ΠπT π
P V −ΠπT π
P V ′∥νπ ≤β∥V −V ′∥νπ.
Unlike in linear TD for MDP [57], Tamar et al. [53] make an additional assumption (Assumption 2)
to establish the contraction property of ΠπT π
P (Proposition 3) for RMDP.
Assumption 2. There exists β ∈(0, 1) with γps,a(s′) ≤βp◦
s,a(s′) ∀s, s′ ∈S, ∀a ∈A, and ∀p ∈P.
Proposition 2 (Prop.3 in [53]). Under Assumption 2, ΠπT π
P is a β-contraction w.r.t. ∥· ∥vπ.
The implicit uncertainty set P of double-sampling satisfies Assumption 2 for small δ, and thus
guarantees contraction of ΠπT π
P . For example, for m = 2 as in (4), a δ <
1−γ
2γ
is sufficient.
However, simply taking a small radius δ is not a panacea for all uncertainty sets:
Proposition 3. For any f-divergence and radius δ > 0, there exists a geometrically mixing nominal
model such that the f-divergence defined uncertainty set violates Assumption 2.
On the other hand, Assumption 2, though well-accepted [53, 45, 41], may not be necessary. The
proposed IPM uncertainty set relates robustness and regularization with an explicit formula for ro-
bust Bellman operator as in (6). The contraction behavior of ΠπT π
P for IPM uncertainty set can be
established without Assumption 2:
6
Lemma 1. For IPM uncertainty set with radius δ < λmin(Ψ⊤DπΨ) 1−γ
γ , there exists β < 1 that
ΠπT π
P is a β-contraction mapping w.r.t. norm ∥· ∥vπ.
Since the contraction of ΠπT π
P is obtained by RMDP under DS or IPM uncertainty sets with small
radius δ, we have the first finite sample guarantee of RLTD by recent advances in Markovian stochas-
tic approximation [10]:
Theorem 3 (Informal convergence of robust critic: Details in Appendix C). RLTD with step sizes
αk = Θ(1/k) satisfies E[∥wK −wπ∥2] = ˜O( 1
K ).
6
Robust Natural Actor
The robust natural actor updates the policy parameter θ along an ascent direction that improves the
value via preconditioning through the KL-divergence KL(p, q) := ⟨p, log(p/q)⟩. It has been well
explored for natural policy gradient (NPG)-like algorithms, such as TRPO and PPO in canonical
RL. The ascent direction is obtained by the policy gradient theorem in canonical MDP. We therefore
first discuss the policy gradient for RMDP, where policy πθ is differentiably parameterized by θ.
The robust value V πθ
P (ρ) =: J(θ) is typically Lipschitz under proper parameterization [61], and is
therefore differentiable a.e. by Rademacher’s theorem [14]. Where it is not differentiable, a Fr´echet
supergradient ∇J of J exists if lim supθ′→0
J(θ′)−J(θ)−⟨∇J(θ),θ′−θ⟩
∥θ′−θ∥
≤0 [25]. The following con-
tains the policy gradient theorem for canonical RL as a special case:
Lemma 2 (Policy supergradient). For a policy π = πθ that is differentiable w.r.t. parameter θ,
∇θV π(ρ) =
Es∼dπ,κπ
ρ
Ea∼πs[Qπ(s, a)∇θ log π(a|s)]
1 −γ
=
Es∼dπ,κπ
ρ
Ea∼πs[Aπ(s, a)∇θ log π(a|s)]
1 −γ
is a Fr´echet supergradient of V π(ρ), where κπ is the worst-case transition kernel w.r.t. π.
We consider log-linear policies πθ(a|s) =
exp(ϕ(s,a)⊤θ)
P
a′ exp(ϕ(s,a′)⊤θ), where ϕ(s, a) ∈Rd is the feature
vector and θ ∈Rd is the policy parameter. (The general policy class is treated in Appendix D).
In canonical RL, the training and testing environments follow the same nominal transition p◦. NPG
updates the policy by θ ←θ + ηFρ(θ)†∇θV πθ
p◦(ρ), where Fρ(θ)† is the Moore-Penrose inverse
of the Fisher information matrix Fρ(θ) := E(s,a)∼d
πθ,p◦
ρ
◦πθ[∇θ log πθ(a|s) (∇θ log πθ(a|s))⊤].
An “equivalent” Q-NPG was proposed in [3] to update the policy by θ ←θ + ηu′, where
u′ = arg minu E(s,a)∼dπ,p◦
ρ
◦π[(Qπ
p◦(s, a) −u⊤ϕ(s, a))2]. Note that u determines a Q value func-
tion approximation Qu(s, a) := ϕ(s, a)⊤u, which is compatible with the log-linear policy class
[52]. Since Qπ contains the information on the ascent direction as suggested by Lemma 2, the
Q-NPG update can be viewed as inserting the best compatible Q-approximation Qu′ for the policy.
We adopt the Q-NPG to robust RL and propose the Robust Q-Natural Policy Gradient (RQNPG)
(Algorithm 3). Note that unlike canonical RL, where Qπ
p◦can be estimated directly from a sample
trajectory of executing π in model p◦, the robust Qπ is hard to estimate with samples from p◦. A
value function approximation Vw from the critic comes to help, as we can approximate the robust Q
function via Qw(s, a) = r(s, a) + infp∈Ps,a p⊤Vw, which exactly matches Qπ if Vw = V π. The
RQNPG obtains information on Qπ by first approximating it via a critic value Vw-guided function
Qw, and then estimating Qw by a policy-compatible robust Q-approximation Qu.
As shown in Algorithm 3, RQNPG estimates a compatible Q-approximation QuN by iteratively
performing sampling and updating procedures, where the sampling procedure follows that of
the RLTD (Algorithm 2).
The update procedure essentially approaches the minimizer uπ
w :=
arg minu E(s,a)∼νπ◦π

(Qw(s, a) −u⊤ϕ(s, a))2
by stochastic approximation with step size ζn
(stochastic gradient descent with Markovian data), since conditioned on (s, a), ˆT π
P Vw ((3) or (6)) is
an unbiased estimator for the Q function approximation Qw.
Now we look at a specific update θt+1 = RQNPG(θt, ηt, wt, N) with ζn = Θ(1/n), where wt =
RLTD(πθt, K). The following theorem shows an approximate policy improvement property of the
RQNPG update:
7
Algorithm 3: Robust Q-Natural Policy Gradient (RQNPG)
Input: θ, η, w, N
Initialize: u0, s0
for n = 0, 1, . . . , N −1 do
Sample an ∼πθ(·|sn), yn+1 according to p◦
sk,ak and determine sn+1 from y′
n+1
Update un+1 = un + ζnϕ(sn, an)
h
( ˆT π
P Vw)(sn, an, yn+1) −ϕ(sn, an)⊤un
i
// For DS: yn+1 = s′
1:m
i.i.d.
∼p◦
sn,an, sn+1 ∼Unif(yn+1) and ˆT π
P in (3)
// For IPM: yn+1 = sn+1 ∼p◦
sn,an and ˆT π
P in (6)
Return: θ + ηuN
0
10
20
30
40
50
'leg_joint_stiffness' values (default=0.0)
500
1000
1500
2000
2500
3000
3500
4000
Cumulative reward
RNAC-PPO(DS)
PPO
(a) Double sampling, Hopper
0
10
20
30
40
'foot_joint_stiffness' values (default=0.0)
2500
3000
3500
4000
4500
5000
Cumulative reward
RNAC-PPO(DS)
PPO
(b) Double sampling, Walker
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Bound on back actuator range (default=1.0)
500
0
500
1000
1500
2000
Cumulative reward
RNAC-PPO(DS)
PPO
(c) Double sampling, HalfCheetah
0
10
20
30
40
50
'leg_joint_stiffness' values (default=0.0)
1000
1500
2000
2500
3000
3500
Cumulative reward
RNAC-PPO(IPM)
PPO
(d) IPM, Hopper
0
10
20
30
40
'foot_joint_stiffness' values (default=0.0)
2500
3000
3500
4000
4500
5000
5500
Cumulative reward
RNAC-PPO(IPM)
PPO
(e) IPM, Walker
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Bound on back actuator range (default=1.0)
500
0
500
1000
1500
2000
2500
Cumulative reward
RNAC-PPO(IPM)
PPO
(f) IPM, HalfCheetah
Figure 1: Cumulative rewards of RNAC-PPO(DS/IPM) and PPO on (a-c) stochastic MuJoCo Envs
and (d-f) deterministic MuJoCo Envs under perturbation.
Theorem 4 (Approximate policy improvement). For any t ≥0, we know
V πt+1(ρ) ≥V πt(ρ) +
KL
d
πt+1,κπt+1
ρ
(πt, πt+1) + KL
d
πt+1,κπt+1
ρ
(πt+1, πt)
(1 −γ)ηt
−
ϵt
1 −γ ,
(7)
where KLν(π, π′) := P
s ν(s)KL(π(·|s), π′(·|s)) ≥0 and E[ϵt] = ˜O(
1
√
N +
1
√
K ) + O(ϵbias).
7
Experimental Results
We demonstrate the robustness of our RNAC approach (Algorithm 1) with Double-Sampling (DS)
and IPM uncertainty sets on MuJoCo simulation environments [56]. We also perform real-world
evaluations using TurtleBot [5], a mobile robot, on navigation tasks under action/policy perturba-
tion. We implement a practical version RNAC using neural network function approximation, with
the robust critic minimizing squared robust TD-error and the robust natural actor performing a robust
proximal policy optimization (PPO) (see Algorithm 4 in Appendix A for details). We call this RNAC
algorithm as RNAC-PPO and compare it with the canonical PPO algorithm [48]. Additional exper-
imental results and details are deferred to Appendix A. We provide code with detailed instructions
at https://github.com/tliu1997/RNAC.
7.1
MuJoCo Environments
We present the experimental results for perturbed MuJoCo Envs (Hopper-v3, Walker2d-v3 and
HalfCheetah-v3) by changing their physical parameters (leg joint stiffness, foot joint stiffness and
8
0.0
0.5
1.0
1.5
2.0
TurtleBot position x
0.0
0.5
1.0
1.5
2.0
TurtleBot position y
RNAC-PPO(DS)
PPO
target
(a) DS, Unif(−0.5, 0.5)
0.0
0.5
1.0
1.5
2.0
TurtleBot position x
0.0
0.5
1.0
1.5
2.0
TurtleBot position y
RNAC-PPO(DS)
PPO
target
(b) DS, Unif(1, 1.5)
0.0
0.5
1.0
1.5
2.0
TurtleBot position x
0.0
0.5
1.0
1.5
2.0
TurtleBot position y
RNAC-PPO(IPM)
PPO
target
(c) IPM, deterministic
0.0
0.5
1.0
1.5
2.0
TurtleBot position x
0.0
0.5
1.0
1.5
2.0
TurtleBot position y
RNAC-PPO(IPM)
PPO
target
(d) IPM, Unif(1, 1.5)
Figure 2: (a-b) show trajectories under balanced (nominal) / unbalanced noise perturbed envs; (c-d)
show trajectories under deterministic (nominal) / unbalanced noise perturbed envs.
back actuator range).
We compare the performance of RNAC-PPO with that of the canonical
PPO algorithm in Fig. 1, where the curves are averaged over 30 different seeded runs and the
shaded region indicates the mean ± standard deviation. RNAC-PPO and PPO are trained with
data sampled from the nominal models (e.g., leg joint stiffness= 0.0, foot joint stiffness= 0.0, and
back actuator range= 1.0).
DS Uncertainty Set: MuJoCo environments have deterministic models. However, most uncertainty
sets are only reasonable for stochastic models, e.g., f-divergence uncertainty sets. So, we add a
uniform actuation noise ∼Unif[-5e-3, 5e-3] in constructing stochastic MuJoCo environments as in
[65]. We use m = 2 and radius δ = 1/6 for RNAC-PPO in (4). The choice of m = 2 makes the
training time of RNAC-PPO with DS uncertainty set and PPO comparable. Fig. 1a-1c demonstrate
the robust performance of RNAC-PPO with Double-Sampling (DS) uncertainty sets in stochastic
MuJoCo environments. Compared to PPO, the cumulative rewards of RNAC-PPO decay much
slower as perturbations increase though they are slightly lower at the beginning (i.e., for the nominal
model). The slight drop in initial performance may stem from optimizing the robust value function
(1) under the worst-case transition models instead of the nominal models.
IPM Uncertainty Set: Since IPM with robust Bellman operator in (6) establishes robustness by
negative regularization, it applies to environments with deterministic transition kernels. We select
δ = 10−5 in (6) and evaluate RNAC-PPO with IPM uncertainty sets on deterministic MuJoCo envi-
ronments in Fig. 1d-1f. RNAC-PPO has more robust behaviors with slow cumulative reward decay
as perturbations increase. Notably, RNAC-PPO enjoys similar and sometimes even better initial
performance on the nominal model compared to PPO, which we believe is due to the regularization
of neural network parameters suggested by IPM (6) that can potentially improve neural network
training.
7.2
TurtleBot Experiments
We demonstrate the robustness of the policy learned by RNAC-PPO on a real-world mobile robot
(Fig. 3). We consider a navigation task as illustrated in Fig. 2, where the goal of the policy is to
navigate the TurtleBot from the origin (0, 0) to a target region centered at (1, −1).
Figure 3: TurtleBot Burger
DS Uncertainty Set: We train RNAC-PPO and PPO on a stochastic
nominal model with balanced actuation noise [54], and test the learned
policies in the nominal model (Fig. 2a) and an unbalanced perturbed
model (Fig. 2b). The policies learned by RNAC-PPO can reach the
target region in both the nominal model and the perturbed model, while
policies learned by PPO are fragile to perturbation and may not reach
the target, as shown in Fig. 2b.
IPM Uncertainty Set: The robustness of the policies learned by RNAC-
PPO trained on a deterministic nominal model is demonstrated in
Fig. 2c and 2d, where the RNAC-PPO learned policies drive the robot
to the target under perturbation, while the PPO-learned policies fail.
A video of this real-world demonstration TurtleBot is available at
[Video Link].
9
8
Conclusion and Future Works
We have proposed two novel uncertainty sets based on double sampling and an integral probability
metric, respectively, that are compatible with function approximation for large-scale robust RL. We
propose a robust natural actor-critic algorithm, which to the best of our knowledge is the first policy-
based approach for robust RL under function approximation with provable guarantees on learning
the optimal robust policy. We demonstrate the robust performance of the proposed algorithm in
multiple perturbed MuJoCo environments and a real-world TurtleBot navigation task.
Although several new theoretical and empirical results about large-scale robust RL are presented
in this paper, there are still many open questions that need to be addressed. Current work focuses
on the (s, a)-rectangular RMDP (Def. 1). We leave extensions to more general s-rectangular and
non-rectangular RMDP for future works. Some theoretical analysis in this paper partly relies on
Assumption 2. Though it is commonly made and accepted in theoretical works as discussed in
paragraphs after Assumption 2, it is not a necessary condition. Further exploration of this can
potentially lead to more theoretical advances. Another natural future direction is to extend current
results to more complex settings such as robust constrained RL and robust multi-agent RL, following
recent developments of policy gradient-based approaches in safe RL [68, 32] and multi-agent RL
[67, 50].
Acknowledgement
Dileep Kalathil’s work is partially supported by the funding from the U.S. National Science Foun-
dation (NSF) grant NSF-CAREER-EPCN-2045783.
P. R. Kumar’s work is partially supported by the US Army Contracting Command under W911NF-
22-1-0151 and W911NF2120064, US National Science Foundation under CMMI-2038625, and US
Office of Naval Research under N00014-21-1-2385. The views expressed herein and conclusions
contained in this document are those of the authors and should not be interpreted as representing the
views or official policies, either expressed or implied, of the U.S. NSF, ONR, ARO, or the United
States Government. The U.S. Government is authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copyright notation herein.
Portions of this research were conducted with the advanced computing resources provided by Texas
A&M High Performance Research Computing.
References
[1] Mohammed Amin Abdullah, Hang Ren, Haitham Bou Ammar, Vladimir Milenkovic, Rui Luo,
Mingtian Zhang, and Jun Wang. Wasserstein robust reinforcement learning. arXiv preprint
arXiv:1907.13196, 2019.
[2] Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory
and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, pages 10–4, 2019.
[3] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. The Journal of Machine
Learning Research, 22(1):4431–4506, 2021.
[4] Carlo Alfano and Patrick Rebeschini. Linear convergence for natural policy gradient with
log-linear policy parametrization. arXiv preprint arXiv:2209.15382, 2022.
[5] Robin Amsters and Peter Slaets. Turtlebot 3 as a Robotics Education Platform, pages 170–181.
01 2020.
[6] Jose Blanchet, Miao Lu, Tong Zhang, and Han Zhong. Double pessimism is provably efficient
for distributionally robust offline reinforcement learning: Generic algorithm and robust partial
coverage. arXiv preprint arXiv:2305.09659, 2023.
[7] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
10
[8] Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global conver-
gence of natural policy gradient methods with entropy regularization. Operations Research,
70(4):2563–2578, 2022.
[9] Zaiwei Chen, Sajad Khodadadian, and Siva Theja Maguluri. Finite-sample analysis of off-
policy natural actor–critic with linear function approximation. IEEE Control Systems Letters,
6:2611–2616, 2022.
[10] Zaiwei Chen, Sheng Zhang, Thinh T Doan, John-Paul Clarke, and Siva Theja Maguluri.
Finite-sample analysis of nonlinear stochastic approximation with applications in reinforce-
ment learning. Automatica, 146:110623, 2022.
[11] Esther Derman, Daniel J Mankowitz, Timothy A Mann, and Shie Mannor. Soft-robust actor-
critic policy-gradient. arXiv preprint arXiv:1803.04848, 2018.
[12] John C Duchi and Hongseok Namkoong. Learning models with uniform performance via
distributionally robust optimization. The Annals of Statistics, 49(3):1378–1406, 2021.
[13] Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust
rl problems. In International Conference on Learning Representations.
[14] Herbert Federer. Geometric measure theory. Springer, 2014.
[15] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in
actor-critic methods.
In International conference on machine learning, pages 1587–1596.
PMLR, 2018.
[16] Vineet Goyal and Julien Grand-Clement. Robust markov decision processes: Beyond rectan-
gularity. Mathematics of Operations Research, 48(1):203–226, 2023.
[17] Julien Grand-Cl´ement and Christian Kroer. Scalable first-order methods for robust mdps. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12086–12094,
2021.
[18] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. In International
conference on machine learning, pages 1861–1870. PMLR, 2018.
[19] Garud N Iyengar.
Robust dynamic programming.
Mathematics of Operations Research,
30(2):257–280, 2005.
[20] Nick Jakobi, Phil Husbands, and Inman Harvey. Noise and the reality gap: The use of simu-
lation in evolutionary robotics. In Advances in Artificial Life: Third European Conference on
Artificial Life Granada, Spain, June 4–6, 1995 Proceedings 3, pages 704–720. Springer, 1995.
[21] Bai Jiang, Qiang Sun, and Jianqing Fan. Bernstein’s inequality for general markov chains.
arXiv preprint arXiv:1805.10721, 2018.
[22] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pages 2137–
2143. PMLR, 2020.
[23] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learn-
ing. In Proceedings of the Nineteenth International Conference on Machine Learning, pages
267–274, 2002.
[24] Diederik P Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
arXiv
preprint arXiv:1412.6980, 2014.
[25] A Ya Kruger. On fr´echet subdifferentials. Journal of Mathematical Sciences, 116(3):3325–
3358, 2003.
11
[26] Yufei Kuang, Miao Lu, Jie Wang, Qi Zhou, Bin Li, and Houqiang Li. Learning robust pol-
icy against disturbance in transition dynamics via state-conservative policy optimization. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 7247–7254,
2022.
[27] Navdeep Kumar, Esther Derman, Matthieu Geist, Kfir Levy, and Shie Mannor. Policy gradient
for s-rectangular robust markov decision processes. arXiv preprint arXiv:2301.13589, 2023.
[28] Navdeep Kumar, Kfir Levy, Kaixin Wang, and Shie Mannor. Efficient policy iteration for
robust markov decision processes via regularization. arXiv preprint arXiv:2205.14327, 2022.
[29] Yan Li, Tuo Zhao, and Guanghui Lan.
First-order policy optimization for robust markov
decision process. arXiv preprint arXiv:2209.10579, 2022.
[30] Zhipeng Liang, Xiaoteng Ma, Jose Blanchet, Jiheng Zhang, and Zhengyuan Zhou. Single-
trajectory distributionally robust reinforcement learning. arXiv preprint arXiv:2301.11721,
2023.
[31] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval
Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning.
arXiv preprint arXiv:1509.02971, 2015.
[32] Tao Liu, Ruida Zhou, Dileep Kalathil, PR Kumar, and Chao Tian. Policy optimization for
constrained mdps with provable fast global convergence. arXiv preprint arXiv:2111.00552,
2021.
[33] Zijian Liu, Qinxun Bai, Jose Blanchet, Perry Dong, Wei Xu, Zhengqing Zhou, and Zhengyuan
Zhou. Distributionally robust q-learning. In International Conference on Machine Learning,
pages 13623–13643. PMLR, 2022.
[34] Xiaoteng Ma, Zhipeng Liang, Li Xia, Jiheng Zhang, Jose Blanchet, Mingwen Liu, Qianchuan
Zhao, and Zhengyuan Zhou. Distributionally robust offline reinforcement learning with linear
function approximation. arXiv preprint arXiv:2209.06620, 2022.
[35] Shie Mannor, Ofir Mebel, and Huan Xu. Robust mdps with k-rectangular uncertainty. Mathe-
matics of Operations Research, 41(4):1484–1509, 2016.
[36] Shie Mannor, Duncan Simester, Peng Sun, and John N Tsitsiklis. Bias and variance approxi-
mation in value function estimates. Management Science, 53(2):308–322, 2007.
[37] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global con-
vergence rates of softmax policy gradient methods. In International Conference on Machine
Learning, pages 6820–6829. PMLR, 2020.
[38] Alfred M¨uller. Integral probability metrics and their generating classes of functions. Advances
in applied probability, 29(2):429–443, 1997.
[39] Ariel Neufeld and Julian Sester. Robust q-learning algorithm for markov decision processes
under wasserstein uncertainty. arXiv preprint arXiv:2210.00898, 2022.
[40] Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncer-
tain transition matrices. Operations Research, 53(5):780–798, 2005.
[41] Kishan Panaganti and Dileep Kalathil. Robust reinforcement learning using least squares pol-
icy iteration with provable performance guarantees. In International Conference on Machine
Learning, pages 511–520. PMLR, 2021.
[42] Kishan Panaganti and Dileep Kalathil. Sample complexity of robust reinforcement learning
with a generative model. In International Conference on Artificial Intelligence and Statistics
(AISTATS), pages 9582–9602, 2022.
[43] Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. Robust rein-
forcement learning using offline data. In Advances in Neural Information Processing Systems,
pages 32211–32224, 2022.
12
[44] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real
transfer of robotic control with dynamics randomization. In 2018 IEEE international confer-
ence on robotics and automation (ICRA), pages 3803–3810. IEEE, 2018.
[45] Aurko Roy, Huan Xu, and Sebastian Pokutta. Reinforcement learning under model mismatch.
Advances in neural information processing systems, 30, 2017.
[46] Reazul Hasan Russel and Marek Petrik. Beyond confidence regions: Tight bayesian ambiguity
sets for robust mdps. Advances in Neural Information Processing Systems, 2019.
[47] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust
region policy optimization. In International conference on machine learning, pages 1889–
1897. PMLR, 2015.
[48] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[49] Laixi Shi and Yuejie Chi. Distributionally robust model-based offline reinforcement learning
with near-optimal sample complexity. arXiv preprint arXiv:2208.05767, 2022.
[50] Youbang Sun, Tao Liu, Ruida Zhou, PR Kumar, and Shahin Shahrampour.
Provably fast
convergence of independent natural policy gradient for markov potential games. arXiv preprint
arXiv:2310.09727, 2023.
[51] Niko S¨underhauf, Oliver Brock, Walter Scheirer, Raia Hadsell, Dieter Fox, J¨urgen Leitner, Ben
Upcroft, Pieter Abbeel, Wolfram Burgard, Michael Milford, et al. The limits and potentials of
deep learning for robotics. The International journal of robotics research, 37(4-5):405–420,
2018.
[52] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. Advances in neural infor-
mation processing systems, 12, 1999.
[53] Aviv Tamar, Shie Mannor, and Huan Xu. Scaling up robust mdps using function approxima-
tion. In International conference on machine learning, pages 181–189. PMLR, 2014.
[54] Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and
applications in continuous control. In International Conference on Machine Learning, pages
6215–6224. PMLR, 2019.
[55] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel.
Domain randomization for transferring deep neural networks from simulation to the real world.
In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages
23–30. IEEE, 2017.
[56] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages
5026–5033. IEEE, 2012.
[57] John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function
approximation. Advances in neural information processing systems, 9, 1996.
[58] Qiuhao Wang, Chin Pang Ho, and Marek Petrik. On the convergence of policy gradient in
robust mdps. arXiv preprint arXiv:2212.10439, 2022.
[59] Shengbo Wang, Nian Si, Jose Blanchet, and Zhengyuan Zhou. A finite sample complexity
bound for distributionally robust q-learning. In International Conference on Artificial Intelli-
gence and Statistics, pages 3370–3398. PMLR, 2023.
[60] Yue Wang and Shaofeng Zou. Online robust reinforcement learning with model uncertainty.
Advances in Neural Information Processing Systems, 34:7193–7206, 2021.
[61] Yue Wang and Shaofeng Zou. Policy gradient method for robust reinforcement learning. In
International Conference on Machine Learning, pages 23484–23526. PMLR, 2022.
13
[62] Wolfram Wiesemann, Daniel Kuhn, and Berc¸ Rustem. Robust Markov decision processes.
Mathematics of Operations Research, 38(1):153–183, 2013.
[63] Huan Xu and Shie Mannor. Distributionally robust Markov decision processes. In Advances
in Neural Information Processing Systems, pages 2505–2513, 2010.
[64] Zaiyan Xu, Kishan Panaganti, and Dileep Kalathil. Improved sample complexity bounds for
distributionally robust reinforcement learning. In International Conference on Artificial Intel-
ligence and Statistics (AISTATS), pages 9728–9754, 2023.
[65] Mengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Dichotomy of control:
Separating what you can control from what you cannot. arXiv preprint arXiv:2210.13435,
2022.
[66] Wenhao Yang, Liangyu Zhang, and Zhihua Zhang. Toward theoretical understandings of robust
markov decision processes: Sample complexity and asymptotics. The Annals of Statistics,
50(6):3223–3248, 2022.
[67] Runyu Zhang, Jincheng Mei, Bo Dai, Dale Schuurmans, and Na Li. On the global convergence
rates of decentralized softmax gradient play in markov potential games. Advances in Neural
Information Processing Systems, 35:1923–1935, 2022.
[68] Ruida Zhou, Tao Liu, Dileep Kalathil, PR Kumar, and Chao Tian. Anchor-changing regular-
ized natural policy gradient for multi-objective reinforcement learning. Advances in Neural
Information Processing Systems, 2022.
[69] Zhengqing Zhou, Qinxun Bai, Zhengyuan Zhou, Linhai Qiu, Jose Blanchet, and Peter Glynn.
Finite-sample regret bound for distributionally robust offline tabular reinforcement learning. In
International Conference on Artificial Intelligence and Statistics, pages 3331–3339, 2021.
14
A
Experimental Details and Additional Experimental Results
In this section, we provide details of the RNAC algorithm (Algorithm 1) implemented in the experi-
ments – robust natural actor-critic proximal policy optimization (RNAC-PPO) algorithm (Algorithm
4). We also demonstrate further experimental results evaluated on different perturbations of physical
hyperparameters in Hopper-v3, Walker2d-v3, and HalfCheetah-v3 from OpenAI Gym [7] compared
with soft actor-critic (SAC) [18] and soft-robust [11] PPO (SRPPO). Finally, we introduce experi-
mental details of the TurtleBot navigation task.
A.1
RNAC-PPO Algorithm
We provide the RNAC-PPO algorithm in Algorithm 4, where the robust critic is minimizing squared
robust TD error (MSRTDE) and the robust natural actor is performing the clipped version of robust
PPO (RPPO) for computational efficiency, and we name it RNAC-PPO for simplicity. We implement
RNAC-PPO employing neural network (NN) function approximation, where the robust natural actor
is utilizing a neural Gaussian policy [48] with two hidden layers of width 64, and the value function
in the robust critic is also parameterized by an NN with two hidden layers of width 64. Compared
with the canonical empirical Bellman operator in the PPO algorithm, we adopt the robust empirical
Bellman operator in the Robust PPO algorithm based on the double-sampling (DS) uncertainty set
and the integral probability metric (IPM) uncertainty set, which can be efficiently computed. For
a better comparison with SAC in the next subsection, we adopt several modifications (e.g., state
normalization, reward scaling, gradient clip, etc) in the implementation of PPO-based algorithms to
improve their performance under the nominal model. Note that for fairness we employ the same
modification for both PPO and RNAC-PPO algorithms.
We use the same hyperparameters across different MuJoCo environments. Specifically, we select
γ = 0.99 for the discount factor, ηt = αt = 3 × 10−4 for learning rates of both actor and critic
updates implemented by ADAM [24], T = 3 × 106 for the maximum training steps, and B = 2048
for the batch size.
Algorithm 4: RNAC-PPO – MSRTDE + RPPO
Input: T, B, η0:T −1, α0:T −1
Initialize: θ0 for policy parameterization, w−1 for value function approximation
for t = 0, 1, . . . , T −1 do
Collect set of trajectories Dt until |Dt| = B by running policy πθt under p◦
Update value function by minimizing mean-squared TD error with learning rate αt
wt = arg min
w
1
|Dt|
X
(s,a,y′)∈Dt
h
Vw(s) −( ˆT π
P Vwt−1)(s, a, y′)
i2
.
// For DS: y′ = s′
1:m
i.i.d.
∼p◦
s,a, s′ ∼Unif(y′) and ˆT π
P in (3)
// For IPM: y′ = s′ ∼p◦
s,a and ˆT π
P in (6)
Compute
ˆAt(s, a) = ( ˆT π
P Vwt)(s, a, y′) −Vwt(s), ∀(s, a, y′) ∈Dt
Update policy via maximizing robust PPO objective with learning rate ηt
θt+1 = arg max
θ
1
|Dt|
X
(s,a,y′)∈Dt
min
 πθ(a|s)
πθt(a|s)
ˆAt(s, a), clip(ϵ, ˆAt(s, a))

,
where clip(ϵ, A) =
(1 + ϵ)A, if A ≥0
(1 −ϵ)A, if A < 0
Output: θT
15
0
20
40
60
80
100
Percentage change in 'leg_joint_damping'
2000
2250
2500
2750
3000
3250
3500
3750
Cumulative reward
RNAC-PPO(DS)
PPO
(a) DS, Hopper-v3
0
10
20
30
40
50
'leg_joint_stiffness' values (default=0.0)
2000
2500
3000
3500
4000
4500
5000
Cumulative reward
RNAC-PPO(DS)
PPO
(b) DS, Walker-v3
0
10
20
30
40
50
60
70
Percentage change in front 'joint_stiffness'
1000
1500
2000
2500
3000
3500
Cumulative reward
RNAC-PPO(DS)
PPO
(c) DS, HalfCheetah-v3
0
20
40
60
80
100
Percentage change in 'leg_joint_damping'
1500
2000
2500
3000
3500
Cumulative reward
RNAC-PPO(IPM)
PPO
(d) IPM, Hopper-v3
0
10
20
30
40
50
'leg_joint_stiffness' values (default=0.0)
3000
3500
4000
4500
5000
5500
Cumulative reward
RNAC-PPO(IPM)
PPO
(e) IPM, Walker-v3
0
10
20
30
40
50
60
70
Percentage change in front 'joint_stiffness'
0
500
1000
1500
2000
2500
Cumulative reward
RNAC-PPO(IPM)
PPO
(f) IPM, HalfCheetah-v3
Figure 4: Cumulative rewards of RNAC-PPO(DS/IPM) and PPO on (a-c) stochastic MuJoCo Envs
and (d-f) deterministic MuJoCo Envs under perturbation.
A.2
Perturbed Mujoco Environments with Other Hyperparameters
In this subsection, we provide more experimental results on different perturbations of physical hy-
perparameters (e.g., leg joint damping, leg joint stiffness, and front joint stiffness) in Hopper-v3,
Walker2d-v3, and HalfCheetah-v3. Figure 4 shows that the RNAC-PPO algorithm is consistently
robust compared to the PPO algorithm.
A.3
Comparison with Soft Actor-Critic and Soft-Robust PPO
In Section 7, we demonstrate the robust behavior of the RNAC-PPO algorithm compared with the
PPO algorithm in Figure 1. In this subsection, we add two more baselines for robust algorithms:
soft actor-critic (SAC) [18] and soft-robust [11] PPO (SRPPO). SAC is regarded as one of the robust
baselines since maximum entropy RL (e.g. SAC) was shown to solve some robust RL problems
by maximizing the lower bound on a robust RL objective [13]. Soft-robust RL learns an optimal
policy based on a distribution over an uncertainty set instead of considering the worst-case scenario
[11]. For a fair comparison, we implement the idea of soft robustness into the framework of PPO.
Specifically, we build an uncertainty set wrapping up four environments, including one nominal
environment and three perturbed environments with leg joint stiffness=5.0 (default=0.0), gravity=-
9.50 (default=-9.81), and actuator ctrlrange=(-0.95, 0.95) (default=(-1.0, 1.0)), respectively. Addi-
tionally, we select [0.85, 0.05, 0.05, 0.05] as the distribution over the above uncertainty set for the
SRPPO algorithm.
Figures 5c and 5d show that the cumulative rewards of RNAC-PPO decay much slower compared
with those of PPO, but similar to those of SAC under the perturbation of gravity. This verifies the
claim that SAC can solve some robust RL problems [13]. However, as shown in Figures 5a and
5b, SAC is not a panacea for all robust RL problems. Under the perturbation of leg joint stiffness,
SAC suffers faster cumulative rewards decay compared with RNAC-PPO. Since SRPPO considers
a distribution over an uncertainty set instead of only sampling from the nominal model, it doesn’t
perform well under the nominal model, but it shows a fairly robust behavior in Figures 5a and 5b
when leg joint stiffness values are perturbed.
Since the cumulative rewards of SAC (> 14000) are much higher than those of PPO-based al-
gorithms (< 3000) in HalfCheetah-v3, we only report the results of SAC in this subsection for
Hopper-v3 to prevent the information of robustness from being blurred. Additionally, the training
time of RNAC-PPO, PPO, and SRPPO is similar, which is at least 5 times less than that of SAC to
16
0
10
20
30
40
50
'leg_joint_stiffness' values (default=0.0)
500
1000
1500
2000
2500
3000
3500
4000
Cumulative reward
RNAC-PPO(DS)
PPO
SAC
SRPPO
(a) DS, stochastic Hopper-v3
0
10
20
30
40
50
'leg_joint_stiffness' values (default=0.0)
1000
1500
2000
2500
3000
3500
4000
Cumulative reward
RNAC-PPO(IPM)
PPO
SAC
SRPPO
(b) IPM, deterministic Hopper-v3
50
40
30
20
10
0
Percentage change in 'gravity' (default=0.0)
500
1000
1500
2000
2500
3000
3500
4000
Cumulative reward
RNAC-PPO(DS)
PPO
SAC
SRPPO
(c) DS, stochastic Hopper-v3
50
40
30
20
10
0
Percentage change in 'gravity' (default=0.0)
0
1000
2000
3000
4000
Cumulative reward
RNAC-PPO(IPM)
PPO
SAC
SRPPO
(d) IPM, deterministic Hopper-v3
Figure 5: Cumulative rewards of RNAC-PPO(DS/IPM), PPO, SAC, and SRPPO on perturbed
Hopper-v3 environments.
end the training of 3 million steps. This is due to the fact that PPO-based algorithms require fewer
updates for critic V , while SAC requires more updates for critic Q.
A.4
TurtleBot Experiment Details
The goal of this TurtleBot navigation task is to guide the robot to any desired target within a 2-meter
range. The state space is a 2-dimensional continuous space, comprising the distance and relative
orientation between the robot and the target. The robot is fixed to move towards its front with a
linear velocity of 15 cm/s, while its angular velocity is controlled by the algorithm. Action space is
1-dimensional and continuous, ranging from −2 to 2. The action signal is then linearly scaled into
[−1.5 rad/s, 1.5 rad/s]. The reward function is designed to be proportional to the product of distance
and action-scaled orientation between the robot and the target. Hitting the boundary or reaching the
target would cause a reward of -200 or 200 respectively. One trajectory would end when the robot
hits the boundary, reaches the goal, or the elapsed time is more than 150 seconds.
We train both PPO and RNAC-PPO(IPM) under the simulator Gazebo. To introduce stochasticity
into the originally deterministic Gazebo environment, we apply uniform action noise perturbation.
PPO and RNAC-PPO(DS) are trained under a balanced Unif[−0.5, 0.5] noise-perturbed environ-
ment. All algorithms undergo 400 epochs of training, and the policy with the highest speed is saved.
Subsequently, we evaluate all policies in an unbalanced Unif[1.0, 1.5] noise-perturbed environment.
We employ such a high noise since all learned algorithms are aggressive at fast turning, with action
close to the limit of −2 or 2. The trajectories of the robot under Gazebo simulator and a video for
the real-world experiment are demonstrated in Figure 2 and [Video Link].
Specifically, both PPO and RNAC-PPO are implemented by neural networks. The actor network is
defined by the Gaussian policy, with one hidden layer and tanh activation function, projecting the
17
0.0
0.5
1.0
1.5
2.0
TurtleBot position x
0.0
0.5
1.0
1.5
2.0
TurtleBot position y
RNAC-PPO(DS)
PPO
Action Noise
Dynamic Random
Target
(a) DS, Unif(-0.5,0.5)
0.0
0.5
1.0
1.5
2.0
TurtleBot position x
0.0
0.5
1.0
1.5
2.0
TurtleBot position y
RNAC-PPO(DS)
PPO
Action Noise
Dynamic Random
Target
(b) DS, Unif(1,1.5)
0.0
0.5
1.0
1.5
2.0
TurtleBot position x
0.0
0.5
1.0
1.5
2.0
TurtleBot position y
RNAC-PPO(IPM)
PPO
Action Noise
Dynamic Random
Target
(c) IPM, deterministic
0.0
0.5
1.0
1.5
2.0
TurtleBot position x
0.0
0.5
1.0
1.5
2.0
TurtleBot position y
RNAC-PPO(IPM)
PPO
Action Noise
Dynamic Random
Target
(d) IPM, Unif(1,1.5)
Figure 6: Turtlebot’s trajectories of RNAC-PPO(DS/IPM), PPO, dynamic randomization, and action
noise envelope. (ac) are trajectories under a nominal environment, (bd) are trajectories under an
unbalanced testing environment. In (b), target reaching rates are: 100% for RNAC-PPO(DS), 67.5%
for action noise envelope, 9% for dynamic randomization, 0% for PPO. In (d), target reaching rates
are: 100% for RNAC-PPO(IPM), 13% for action noise envelope, 2% for dynamic randomization,
0% for PPO
input state into a feature with dimension 100, then into the 1-dimensional action. Critic network
consists of one hidden layer, with a width 100, and a ReLU activation function. We choose discount
factor as 0.99, learning rate as 2.5 × 10−4, batch size as 64, and optimizer ADAM.
Additionally, we also add more baselines (i.e., dynamics randomization [44] and action noise en-
velope [20]) to demonstrate the robustness of the proposed methods in the real-world TurtleBot
environment. As shown in Figure 6, the proposed algorithm RNAC-PPO (DS / IPM) enjoys higher
target reaching rates (100% / 100%), compared with action noise envelope (67.5% / 13%), dynamic
randomization (9% / 2%), and PPO (0% / 0%), under perturbed testing environments, which illus-
trates the robustness of the RNAC-PPO algorithm.
We end this section by illustrating our hardware configurations. All experimental results are carried
out on a Linux server with 48-core RTX 6000 GPUs, 48-core Intel Xeon 6248R CPUs, and 384 GB
DDR4 RAM.
B
Discussions on Uncertainty Sets
R-contamination For R-contamination [60], the uncertainty set is P = ⊗s,aPs,a, where Ps,a =
{Rq + (1 −R)po
s,a : q ∈∆S}. It can be shown that
inf
p∈Ps,a p⊤V = (1 −R)(p◦
s,a)⊤V + R min
s′ V (s′).
(8)
18
Its corresponding robust Bellman operator requires searching the entire state space S to calculate
the minimum value in V , and thus intractable for large state space.
ℓp norm [28] For any nominal model p◦, the uncertainty set is P = ⊗s,aPs,a with Ps,a = {p :
∥q −p◦
s,a∥p ≤δ, P
s′ q(s′) = 1}, where ∥· ∥p is an ℓp norm with its dual norm ∥· ∥q satisfying
1
p + 1
q = 1, and the domain of q ∈∆S is relaxed to hyperplane P
s′ q(s′) = 1 due to the difficulty
in handling the boundary of ∆S. It was shown [28] that
inf
p∈Ps,a p⊤V = (p◦
s,a)⊤V −min
w ∥V −w1∥q,
(9)
which requires solving a minimization problem via binary search. The minimum value may have a
closed-form representation (c.f. Table 1 in [27]). For example, the minimum value is the average
of V for ℓ2 norm, the median of V for ℓ1 norm, and the average peak of V (i.e., the average of the
maximum and minimum value of V ) for ℓ∞norm. ℓp norm-based uncertainty set is thus intractable
in the large-scale scenario.
f-divergence Given a continuous strict convex function f : R+ →R with f(1) = 0, f-divergence
is defined by df(q, p) = P
s p(s)f( q(s)
p(s)). By distributionally robust optimization literature [12], we
have
inf
p∈Ps,a p⊤V =
sup
λ>0,η∈R
Es′

−λf ∗
−V (s′) −η
λ

−λδ −η

,
(10)
where f ∗is the Fenchel conjugate of f, i.e., f ∗(y) = supx>0(yx −f(x)). Given such a formula,
the robust Bellman operator can be estimated given samples from the nominal model but requires
the optimal dual variables λ, η for each (s, a) pair.
Wasserstein distance Given some metric d(·, ·) on state space S, Wasserstein-σ distance is defined
for any p, q ∈∆S that
Wσ(p, q) := (
inf
µ∈U(p,q) E(s,s′)∼µ[d(s, s′)σ])
1
σ ,
where U(p, q) is the set of all couplings of p and q. Wasserstein RMDP has uncertainty set P =
⊗s,aPs,a with Ps,a = {q : Wσ(q, p◦
s,a) ≤δ} and we have
inf
p∈Ps,a p⊤V = sup
λ>0
X
s′
p◦
s,a(s′) inf
˜s [V (˜s) + λ(d(˜s, s′)σ −δ)].
(11)
The Wasserstein RMDP has been previously studied [1, 26]. Kuang et al. [26] gives a state dis-
turbance view of Wasserstein RMDP, where the robust Bellman operator requires searching the
worst-case state in the vicinity of the next state sample, i.e., inf ˜s:∥˜s−s′∥≤δ V (˜s). Although this ap-
proach can be applied to RMDP with large state space, the theoretical guarantee provided in [26] is
only for policy iteration (planning problem) in the tabular setting (i.e., the robust Bellman operator
is a contraction mapping under ∥· ∥∞).
B.1
Double-Sampling Uncertainty Sets
When action a is taken at state s, the nominal model transits to the next state s′ ∼p0
s,a. It can
be viewed as a double-sampling process that the next state s′ is generated by uniformly sampling
one from m states s′
1, s′
2, . . . , s′
m sampled i.i.d. according to p0
s,a. The transition kernel in the DS
uncertainty set at (s, a)-pair can be viewed as selecting the next state s′ in {s′
1, . . . , s′
m} according
to a distribution α ∈∆(m) that is perturbed from a uniform distribution Unif(m) and potentially
depends on the samples {s′
1, . . . , s′
m}. The DS uncertainty set is implicitly defined by a choice of
m, divergence measure d(·, ·) and radius δ > 0.
Bellman completeness for robust linear MDP under DS uncertainty set: A Q function class
G ⊂RS×A is said to be Bellman complete if it is closed, and for all g ∈G, T ∗g also lies in G,
where
T ∗g(s, a) := r(s, a) + γ
X
s′
ps,a(s′) max
a
g(s′, a).
19
Bellman completeness is critical for reinforcement learning. For RMDP with the Double-Sampling
(DS) uncertainty set, the linear function approximation can satisfy Bellman completeness if the
nominal model is a linear MDP. In linear MDP [22], we have
p◦
s,a(s′) = ϕ(s, a)⊤µ(s′),
r(s, a) = ϕ(s, a)⊤θ,
where µ is a vector of (signed) measures over S. Then for any V ∈RS
r(s, a) + γEs′∼p◦s,a[V (s′)] = r(s, a) + γ
Z
s′ ϕ(s, a)⊤µ(s′)V (s′)ds′
= ϕ(s, a)⊤

θ + γ
Z
s′ µ(s′)V (s′)ds′

∈span(Φ).
For the DS uncertainty set P
=
⊗s,aPs,a determined by m, d(·, ·), δ, let fV (s′
1:m)
:=
infα∈∆[m]:d(α,Unif([m]))≤δ
Pm
i=1 αiV (s′
i), and ¯fV (s′
1) := Es′
2:m
i.i.d.
∼p◦s,a[fV (s′
1:m)]. It then follows
that for any V
r(s, a) + γ
inf
p∈Ps,a p⊤V = r(s, a) + γEs′
1:m
i.i.d.
∼p◦s,a [fV (s′
1:m)]
= ϕ(s, a)⊤θ + γEs′
1∼p◦s,a[ ¯fV (s′
1)] = ϕ(s, a)⊤

θ + γ
Z
s′ µ(s′) ¯fV (s′)ds′

∈span(Φ).
The Bellman completeness for DS-based RMDP with linear MDP nominal model is thus proved.
B.2
Integral Probability Metric Uncertainty Sets
Under the linear value function approximation, Vw(s) = ψ(s)⊤w. Denote Ψ ∈RS×d as the
feature matrix by stacking up ψ(s)⊤, and the value function approximation is a linear regressor
Vw = Ψw ∈RS. Let Ψ = [ψ1, . . . ψd] with ψi ∈RS, without loss of generality, assume ψ1 is an
all-one vector, which corresponds to the bias term of the linear regressor, and Ψ is full column rank.
We propose the IPM dF(p, q) = supf∈F{p⊤f −q⊤f} determined by function class
F = {Ψξ : ξ ∈Rd, ∥ξ∥≤1}
restatement of Eq (5) in matrix form.
The robust Bellman operator T π
P V (2) requires solving the optimization infq∈Ps,a q⊤V , which is
equivalent to
minq∈∆S q⊤V
s.t. supf∈F q⊤f −(p◦
s,a)⊤f ≤δ.
We relax the domain constraint q ∈∆S to P
s q(s) = 1 and define Ps,a = {q : dF(q, p◦
s,a) ≤
δ, P
s q(s) = 1}. This relaxation omits the boundary effect of ∆S, which facilitates the following
analysis. The relaxation is also made for ℓp norm-based uncertainty set [28], and one can argue that
it does not introduce any relaxation error for p◦
s,a(s′) > 0, ∀s, a, s′ and small δ.
Restatement of Proposition 1. For the IPM with F in (5), we have infq∈Ps,a q⊤Vw = (p◦
s,a)⊤Vw−
δ∥w2:d∥.
Proof of Proposition 1. Denote u = q −p◦
s,a. The infq∈Ps,a q⊤V under the relaxation is
min
u
(p◦
s,a)⊤Ψw + u⊤Ψw,
s.t. sup
∥ξ∥≤1
u⊤Ψξ ≤δ, u⊤1 = 0.
Since Ψ is full column rank and ψ1 is an all 1 vector, we know {Ψ⊤u : u⊤1 = 0, u ∈RS} = {y ∈
Rd : y1 = 0}. The optimization problem can then be written as
min
y
(p◦
s,a)⊤Ψw + y⊤
2:dw2:d,
s.t.
sup
∥ξ2:d∥≤1
y⊤
2:dξ2:d ≤δ.
The constraint is equivalent to ∥y2:d∥∗≤δ and the optimal value can then be written as (p◦
s,a)⊤Vw−
δ∥w2:d∥, which concludes the proof of Proposition 1.
The IPM has many merits due to its capability to take advantage of the geometry of the domain
(state space) through the function class.
20
C
Robust Critic Analysis
We analyze the robust critic component in the RNAC algorithm (Algorithm 1). This section would
also be of independent interest for the robust policy evaluation problem, where one aims to estimate
the robust value of some policy given Markovian data.
The first two subsections focus on the linear function approximation and prove the main theorems
as stated in Section 5 of the main paper. The last subsection focuses on the general function approx-
imation.
C.1
Linear Robust Value Function Approximation
Setting: We aim to approximate the robust value function V π thorough linear function class
Vw(s) = ψ(s)⊤w,
∀s ∈S.
(12)
The “optimal” linear value approximation Vwπ is the solution of the projected robust Bellman equa-
tion i.e.,
ΠπT π
P Vwπ = Vwπ,
where the projection matrix is Ππ = Ψ(Ψ⊤DπΨ)−1Ψ⊤Dπ with Dπ = diag(νπ) and νπ is the state
stationary distribution of executing policy π on the nominal model p◦.
Definition 3 (Linear value approximation error). ϵV,bias := supπ ∥ΠπV π −V π∥νπ.
wπ is the solution of the projected robust Bellman equation ΠπT π
P Vw = Vw. When ΠπT π
P is
β-contraction w.r.t. ∥· ∥νπ, we have (according to Corollary 4 in [53])
∥Vwπ −V π∥νπ ≤
1
1 −β ∥ΠπV π −V π∥νπ ≤ϵV,bias
1 −β .
(13)
C.1.1
Contraction of Projected Robust Bellman Operator
The contraction property of the projected robust Bellman operator is the key to guaranteeing the
convergence of linear TD algorithms. The contraction property can be guaranteed when Assumption
2 is satisfied, and we show that the DS uncertainty set with a small δ indeed satisfies this assumption.
We refer to a divergence d(·, ·) as C-continuous at uniform distribution Unif([m]), if {α ∈∆[m] :
d(α, Unif([m])) ≤ϵ} ⊆{α ∈∆[m] : ∥α −Unif([m])∥∞≤Cϵ}, ∀ϵ > 0.
Proposition 4. Uncertainty set implicitly defined by double sampling with divergence d(·, ·) that is
C-continuous at uniform distribution Unif([m]) and δ < 1−γ
γmC satisfies Assumption 2.
Proof. For the uncertainty set P implicitly defined by double-sampling with robust Bellman opera-
tor 3
(T π
P V )(s) := Ea∼π(·|s),s′
1:m∼p◦s,a
"
r(s, a) + γ
inf
α∈∆[m]:d(α,Unif([m]))≤δ
m
X
i=1
αiV (s′
i)
#
.
Since d(·, ·) is C-continuous at uniform distribution, take arbitrary (s, a, s′), we know
sup
q∈Ps,a
q(s′) = Es′
1:m∼p◦s,a
"
sup
α∈∆[m]:d(α,Unif(m))≤δ
m
X
i=1
αi1(s′
i = s′)
#
≤Es′
1:m∼p◦s,a
"
sup
α∈∆[m]:∥α−Unif(m)∥∞≤Cδ
m
X
i=1
αi1(s′
i = s′)
#
≤Es′
1:m∼p◦s,a
" m
X
i=1
( 1
m + Cδ)1(s′
i = s′)
#
= p◦
s,a(s′)(1 + mCδ).
To guarantee γ supq∈Ps,a q(s′) ≤βp◦
s,a with some β < 1, we know need (1 + mCδ) < 1
γ , which
can be achieved by δ < 1−γ
γmC .
21
The following theorem shows that choosing a small δ is not a panacea for the well-known f-
divergence uncertainty set. Note that many well-known metrics, such as KL-divergence, total varia-
tion, and χ2-divergence are special cases of f-divergence.
Proposition 5 (Restatement of Proposition 3). For any f-divergence and radius δ > 0, there exists
a geometrically mixing nominal model such that the f-divergence defined uncertainty set violates
Assumption 2.
Proof of Proposition 3. Given a continuous strict convex function f : R+ →R with f(1) = 0, the
f-divergence is defined as df(q, p) = P
s p(s)f( q(s)
p(s)).
Given a nominal model p◦and a radius δ > 0, the f-divergence-based uncertainty set is P
with Ps,a = {q : df(q, p◦
s,a) ≤δ}.
For any f and δ > 0, consider a nominal model
p◦, which has a uniform transition probability, i.e., transits to a uniformly and randomly se-
lected next state at any state-action pair, except for (s, a). The transition probability at (s, a) is
p◦
s,a = (α, 1−α
|S|−1, 1−α
|S|−1, . . . , 1−α
|S|−1), where α is some parameter to be determined later. It is clear
that this nominal model p◦is well mixed for any α ∈[0, 1]. Consider another model q which
coincides with p◦except for states (s, a) with qs,a = (α/γ, 1−α/γ
|S|−1 , 1−α/γ
|S|−1 , . . . , 1−α/γ
|S|−1 ). Note that
qs,a(1)
p◦s,a(1) = 1
γ , thus if q ∈P, the Assumption 2 of β < 1 is violated. Since as α →0, by the continuity
of f and f(1) = 0, we have
df(qs,a, p◦
s,a) =
X
s′
p◦
s,a(s′)f(qs,a(s′)
p◦s,a(s′)) = αf(1/γ) + (1 −α)f(1 −α/γ
1 −α )
= αf(1/γ) + (1 −α)f(1 −1/γ −1
1 −α α) →0 < δ.
Therefore, with a sufficiently small α, we have qs,a ∈Ps,a and clearly q ∈P. Thus there does not
exist a universal choice of δ for f-divergence-based uncertainty set to guarantee Assumption 2.
We next prove Lemma 1, which shows the contraction of robust Bellman operator for IPM-based
RMDP without Assumption 2.
Proof of Lemma 1. Let P π(s′|s) := P
a p◦
s,π(a|s)(s′) be the state transition kernel of executing
policy π on the nominal model p◦. We can view P π ∈RS×S as a matrix. Recall Dπ = diag(νπ),
we have
∥T π
P Ψw −T π
P Ψw′∥νπ =
γP πΨw −γδ∥w2:d∥1 −γP πΨw′ + γδ∥w′
2:d∥1

νπ
≤γ∥Ψw −Ψw′∥νπ + γδ
∥w∥−∥w′∥
 ≤γ∥Ψw −Ψw′∥νπ +
γδ
λmin(Ψ⊤DπΨ)∥Ψw −Ψw′∥νπ.
Since Ππ is a non-expansion mapping w.r.t. ∥· ∥νπ, ΠπT π
P is a contraction mapping if γ(1 +
δ
λmin(Ψ⊤DπΨ)) < 1, which is equivalent to the condition δ < λmin(Ψ⊤DπΨ) 1−γ
γ .
C.1.2
Convergence of Robust Linear TD
As discussed in the paragraph under Assumption 1, robust linear TD (RLTD) is a stochastic approx-
imation (c.f. Section F.3 for a brief overview of stochastic approximation) since empirical operator
ˆT π
P ((3) or (6)) is unbiased with
E(s,a,y′)∼νπ◦π◦p◦
h
ψ(s)
h
( ˆT π
P Vw)(s, a, y′) −ψ(s)⊤w
ii
= Ψ⊤Dπ(T π
P Vw −Vw).
We can then let F(w, x) = ψ(s)
h
( ˆT π
P Vw)(s, a, y′) −ψ(s)⊤w
i
with x = (s, a, y′) and RLTD is
solving the following equation
¯F(w) := Ψ⊤D(T π
P Ψw −Ψw) = 0,
22
through stochastic approximation:
wk+1 = wk + αkF(wk, xk),
xk = (sk, ak, yk+1).
It is clear that {xk} is also a Markov chain with domain X = S × A × Y, Y = Sm for the
double-sampling uncertainty set with m samples and Y = S for the IPM uncertainty set.
Theorem 5 (Formal statement of Theorem 3). Take δ as in Lemma 1 and in Proposition 4, for the
DS uncertain set or the IPM uncertainty set, respectively. Under Assumption 1, RLTD with step
sizes αk = Θ(1/k) guarantees E[∥wK −wπ∥2] = ˜O( 1
K ) and E[∥VwK −Vwπ∥2
νπ] = ˜O( 1
K ).
Proof of Theorem 5. By Proposition 4 and Lemma 1, DS or IPM with small δ can guarantee ΠπT π
P
is a β-contraction w.r.t. ∥· ∥νπ for some β < 1. The theorem can be proved by applying Lemma 12.
For this purpose, we only need to show that Assumption 8 is satisfied for RLTD. We check the three
conditions in Assumption 8 as follows.
1. The geometric mixing property of {xk = (sk, ak, yk+1)}k≥0 is straightforward given geo-
metrically mixed {sk}k≥0.
2. Since r ∈[0, 1] and ∥ψ(s)∥is bounded ∀s ∈S, we have ∥F(0, x)∥= ∥ψ(s)r(s, a)∥≤
∥ψ(s)∥is also bounded for any x = (s, a, y′).
For double-sampling (3) and any x = (s, a, s′
1:m),
F(w, x) = ψ(s)
 
r(s, a) + γ
inf
α∈∆[m]:d(α,Unif(m))≤δ
m
X
i=1
αiVw(s′
i) −ψ(s)⊤w
!
.
Let Ψs′
1:m ∈Rm×d be the matrix by stacking up ψ(s′
1)⊤, . . . , ψ(s′
m)⊤, we have
∥F(w1, x) −F(w2, x)∥
≤γ∥ψ(s)∥
inf
α α⊤Ψs′
1:mw1 −inf
α α⊤Ψs′
1:mw2
 + γ∥ψ(s)ψ(s)⊤(w1 −w2)∥
≤γ∥ψ(s)∥| sup
α α⊤Ψs′
1:m(w1 −w2)| + γ∥ψ(s)∥2∥w1 −w2∥
≤max
s′
2γ∥ψ(s′)∥2∥w1 −w2∥,
where supα, infα are taking in {α ∈∆[m] : d(α, Unif([m])) ≤δ}. Thus there exists L1 =
max(maxs ∥ψ(s)∥, 2γ maxs ∥ψ(s)∥2) that guarantee ∥F(0, x)∥≤L1 and ∥F(w, x) −
F(w′, x)∥≤L1∥w −w′∥for the double-sampling RMDP.
For IPM (6) and any x = (s, a, s′),
F(w, x) = ψ(s)
 r(s, a) + γψ(s′)⊤w −γδ∥w2:d∥−ψ(s)⊤w

.
We have
∥F(w, x) −F(w′, x)∥≤max
s
 (γ + 1)∥ψ(s)∥2 + γδ∥ψ(s)∥

∥w −w′∥,
which
also
satisfies
the
second
item
of
Assumption
8
by
setting
L1
=
max(maxs ∥ψ(s)∥, maxs
 (γ + 1)∥ψ(s)∥2 + γδ∥ψ(s)∥

).
3. Note that for small δ, ΠπT π
P is a β-contraction w.r.t. ∥· ∥νπ. Since there exists wπ with
¯F(wπ) = 0, let D := Dπ = diag(νπ), we have
⟨w −wπ, ¯F(w)⟩= ⟨w −wπ, ¯F(w) −¯F(wπ)⟩
= ⟨w −wπ, Ψ⊤D(T πΨw −T πΨwπ)⟩−∥Ψ(w −wπ)∥2
νπ
= ⟨Ψ⊤DΨ(w −wπ), (Ψ⊤DΨ)−1Ψ⊤D(T πΨw −T πΨwπ)⟩−∥Ψ(w −wπ)∥2
νπ
= ⟨D1/2Ψ(w −wπ), D1/2Ψ(Ψ⊤DΨ)−1Ψ⊤D(T πΨw −T πΨwπ)⟩−∥Ψ(w −wπ)∥2
νπ
≤∥Ψ(w −wπ)∥νπ∥ΠπT π
P Ψw −ΠπT π
P Ψwπ∥νπ −∥Ψ(w −wπ)∥2
νπ
≤−(1 −β)∥Ψ(w −wπ)∥2
νπ ≤−(1 −β)λmin(Ψ⊤DΨ)∥w −wπ∥2,
where the first inequality is due to the Cauchy-Schwarz inequality and the second inequality
is due to the β-contraction property of ΠπT π
P .
23
In addition, we have
E[∥VwK −Vwπ∥2
νπ] = E[(wK −wπ)⊤(Ψ⊤DππΨ)(wK −wπ)]
≤λmax(Ψ⊤DπΨ)E[∥wK −wπ∥2] = ˜O(1/K).
C.2
Extension of General Value Function Approximation
Setting: In the general function approximation setting, we consider a known finite and bounded
function class G ⊂RS to fit the robust value function V π, i.e.,
G ⊂RS,
|G| < ∞,
|g(s)| < ∞, ∀g ∈G, ∀s ∈S.
(14)
Note that IPM-based RMDP with empirical robust Bellman operator (6) only applies to linear func-
tion approximation. We implement it similarly by adding a negative regularization with neural
network approximating the robust values, which also induces robust behavior of the learned policy
as illustrated in the experiments (Section 7). However, it may not directly match any specific uncer-
tainty set. We thus only consider the DS-based RMDP in this general value function approximation
setting.
We define the robust Bellman error as follows.
Definition 4 (Robust Bellman Error). ϵg,bias := maxπ maxg∈G ming′∈G ∥T π
P g −g′∥νπ.
If ϵg,bias = 0, then the robust value is realizable within the function class, i.e., V π ∈G for any
π ∈G.
C.2.1
Fitted Robust Value Evaluation Algorithm
We propose the fitted robust value evaluation (FRVE) in Algorithm 5, a robust version of the fitted
value evaluation commonly used in offline RL. This algorithm first samples a batch of data from
the nominal model, then select the last half as training data for analytical purpose without losing
the order while the Markovian data are close to the stationary distribution due to geometric mixing
Assumption 1, and iteratively solve for a better robust value approximation based on the current
approximation gk and its robust value estimate ˆT π
P (gk).
Algorithm 5: Fitted Robust Value Evaluation (FRVE)
Input: π, K
Initialize: g0, s0
for k = 0, 1, . . . , K −1 do
Sample ak ∼π(·|sk), yk+1 according to p◦
sk,ak, and sk+1 from yk+1
// For DS: yk+1 = s′
1:m
i.i.d.
∼p◦
sk,ak, sk+1 ∼Unif(yk+1) and ˆT π
P in (3)
Let D = {(sk, ak, yk+1)}K−1
k=K/2 be the dataset
for k = 0, 1, . . . , K −1 do
Update gk+1 = arg ming∈G
1
|D|
P
(s,a,y′)∈D

( ˆT π
P gk)(s, a, y′) −g(s)
2
.
Return: gK
Theorem 6 (Convergence of FRVE). For DS RMDP with δ suggested by Proposition 4, T π
P is a
β-contraction mapping for some β < 1. Under Assumption 7, the return of FRVE Algorithm 5 has
E[∥V π −gK∥νπ] ≤βK(Gmax +
1
1 −γ ) + ϵg,stat
1 −β + ϵg,bias
1 −β ,
where Gmax := maxg∈G(( ˆT π
P g)(s, a, y′), ∥T π
P g∥νπ, ∥g∥νπ) and ϵg,stat =
˜O(1/
√
K).
Thus
E[∥V π −gK∥νπ] = ˜O(1/
√
K) + O(ϵg,bias).
24
Proof. By the contraction mapping of T π
P
∥V π −gK∥νπ ≤∥T π
P V π −T π
P gK−1∥νπ + ∥T π
P gK−1 −gK∥νπ
≤β∥V π −gK−1∥νπ + ∥T π
P gK−1 −gK∥νπ
≤
K
X
k=1
βK−k∥T π
P gk−1 −gk∥νπ + βK∥V π −g0∥νπ
≤
K
X
k=1
βK−k∥T π
P gk−1 −gk∥νπ + βK(∥g0∥νπ +
1
1 −γ ).
Note that T π
P gk−1 is a target that gk is an approximation through MSE with Markovian data. If the
data is stationary, i.e., sK/2 ∼νπ, applying Lemma 11 and the union bound over G (taking T π
P g as
a target for each g ∈G), we know with probability at least 1 −δ,
∥T π
P gk−1 −gk∥νπ = O
 
Gmax
r
log(|G|) + log(1/δ)
K
!
+ O(ϵg,bias),
∀k = 1, 2, . . . , K.
Let Pµ be a distribution on D with xK/2 ∼µ, and let Pνπ be a distribution on D with xK/2 ∼νπ.
Take µ as the true distribution of xK/2 according to the data sampling process as in Algorithm 5.
We know ∥µ −νπ∥T V = O(e−K) by Assumption 1, and thus
ED∼Pµ[∥T π
P gk−1 −gk∥νπ] ≤2Gmax∥Pµ −Pνπ∥T V + ED∼Pνπ [∥T π
P gk−1 −gk∥νπ]
= 2Gmax∥µ −νπ∥T V + ED∼Pνπ [∥T π
P gk−1 −gk∥νπ]
= O(Gmaxe−K) + O
 
Gmax
r
log(|G|) + log(K)
K
!
+ O(ϵg,bias).
The proof of the theorem is thus concluded.
D
Robust Natural Actor Analysis
We analyze the robust natural component in the RNAC algorithm (Algorithm 1).
We first discuss the Fr´echet supergradient of the robust value function in the first subsection. The
second subsection focuses on the linear function approximation and proves the main theorems as
stated in Section 5 of the main paper. The last subsection focuses on the general function approxi-
mation.
D.1
Policy Gradient and Performance Difference
The robust value function J(θ) = V πθ(ρ) is in general not differentiable. But since it is Lipschitz
(w.r.t. to Lipschitz policy parameterization), it is differentiable almost everywhere according to
Rademacher’s theorem [14]. At the place not differentiable, Fr´echet supergradient ∇J of J is then
defined as
lim sup
θ′→0
J(θ′) −J(θ) −⟨∇J(θ), θ′ −θ⟩
∥θ′ −θ∥
≤0.
When function J is differentiable, J at any point θ has a unique Fr´echet supergradient, which is the
gradient of J at θ.
Proof of Lemma 2. The Fr´echet supergradient exists for tabular RMDP [29], which is
[∇πV π(ρ)]s,a =
1
1 −γ dπ,κπ
ρ
(s)Qπ(s, a),
∀s, a,
where [∇πV π(ρ)]s,a indicates the (s, a) coordinate of vector ∇πV π(ρ). π = πθ is a policy param-
eterized by θ.
∇θV πθ(ρ) =
X
s,a
[∇πV π(ρ)]s,a∇θπθ(a|s)
25
=
1
1 −γ Es∼dπ,κπ
ρ
Ea∼πs[Qπ(s, a)∇θ log πθ(a|s)]
=
1
1 −γ Es∼dπ,κπ
ρ
Ea∼πs[Aπ(s, a)∇θ log πθ(a|s)],
where the first relation is by the chain rule of supergradient.
D.2
Linear Function Policy Approximation
Setting: This subsection considers the log-linear policy with
πθ(a|s) =
exp
 ϕ(s, a)⊤θ

P
a′ exp(ϕ(s, a′)⊤θ),
∀(s, a) ∈S × A,
(15)
where ϕ(s, a) ∈Rd is some known feature vector and θ ∈Rd is the policy parameter. Let Φ ∈
R|S||A|×d be the feature matrix by stacking up the feature vectors ϕ(s, a)⊤.
Recall the discussion of the proposed RQNPG in Section 6.
We approximate the robust Q
function Qπ via Qw(s, a) = r(s, a) + infp∈Ps,a p⊤Vw given a value function approximation
Vw from the robust critic, and then approximate Qw by a policy-compatible [52] robust Q-
approximation Qu = Φu. In other words, we project Qw onto span(Φ). Denote by Ππ
Φ :=
Φ(Φ⊤diag(νπ ◦π)Φ)−1Φ⊤diag(νπ ◦π) ∈R|S||A|×|S||A| the projection matrix onto space span(Φ)
w.r.t. norm ∥· ∥νπ◦π. We then define the approximation error ϵQ,bias below. When realizable, i.e.,
Qπ ∈span(Φ), the approximation error ϵQ,bias = 0.
Definition 5. ϵQ,bias := maxπ,π′ max
d=d
π′,κπ′
ρ
,dπ∗,κπ
ρ
or νπ∗∥Ππ
ΦQπ −Qπ∥d◦Unif.
We assume a finite relative condition number (Assumption 3) (similar to that in [3]). The relative
condition number is not necessarily related to the size of the state space (details are shown in Remark
6.3 of [3]).
Assumption 3. maxπ,π′ max
d=d
π′,κπ′
ρ
,dπ∗,κπ
ρ
or νπ∗supu
∥Φu∥d◦Unif
∥Φu∥νπ◦π ≤ξ < ∞for some ξ.
Now we look at a specific update θt+1 = RQNPG(θt, ηt, wt, N), where wt = RLTD(πθt, K).
D.2.1
RQNPG One-Step Analysis – Robust Q Function Approximation
In this update θt+1 = RQNPG(θt, ηt, wt, N), where wt = RLTD(πθt, K). RQNPG first approximates
Qwt(s, a) = r(s, a) + γ infp∈Ps,a p⊤Vwt = E[( ˆT π
P Vwt)(s, a, y′)|s, a] by Qu(s, a) = ϕ(s, a)⊤u, as
the caculation of uN in Algorithm 3. Let
ut
∗:= arg min
u E(s,a)∼νt◦πt[(Qwt(s, a) −Qu(s, a))2].
be the optimal approximation for the target Qwt. ut
∗is approximated by stochastic approximation
(c.f. Section F.3 for a brief overview of stochastic approximation) with a mean squared error loss
L(u; Vwt, π) = E(s,a,y′)∼νπ◦π◦p◦

( ˆT π
P Vwt)(s, a, y′) −ϕ(s, a)⊤u
2
.
We know ut
∗is the unique solution of
0 = −∇uL(u; Vwt, π) = E(s,a,y′)∼νπ◦π◦p◦
h
ϕ(s, a)

( ˆT π
P Vwt)(s, a, y′) −ϕ(s, a)⊤u
i
.
Let F(u, x) be the function inside the expectation with x = (s, a, y′), and ¯F(u) be the negative gra-
dient −∇uL(u; V, π). We then solve this stochastic zero point problem by stochastic approximation
as in
un+1 = un + ζnϕ(sn, an)
h
( ˆT π
P Vw)(sn, an, yn+1) −ϕ(sn, an)⊤un
i
= un + ζnF(un, xn),
where xn = (sn, an, yn+1).
Lemma 3 (Convergence of compatible Q function approximation (SGD with Markovian data)).
Under Assumption 1, RQNPG (Algorithm 3) with step sizes ζn = Θ(1/n) guarantees E[∥uN −
ut
∗∥2] = ˜O( 1
N ) and E[∥QuN −Qut
∗∥2
νt◦πt] = ˜O( 1
N ).
26
Proof of Lemma 3. The lemma is implied by Lemma 12. To see this, we only need to show that
Assumption 8 is satisfied. We check the three conditions in Assumption 8 as follows.
1. The geometric mixing property of {xk}k≥0 is straightforward given geometrically mixed
{sk}k≥0.
2. Since ∥ϕ(s, a)∥is bounded ∀(s, a) ∈S ∈A, similar proof follows as in the proof of 5.
3. Since there exists uπ with ¯F(uπ) = 0 and L(u; V, π) is λmin(Es,a[ϕs,aϕ⊤
s,a])-strongly
convex, we have
⟨u −uπ, ¯F(u)⟩= ⟨u −uπ, ¯F(u) −¯F(uπ)⟩
= −⟨u −uπ, ∇L(u; V, π) −∇L(uπ; V, π)⟩≤−λmin(Σνπ◦π)∥u −uπ∥2,
where Σνπ◦π := Es∼νπ,a∼π[ϕs,aϕ⊤
s,a].
Denote by Ππ
Φ := Φ(Φ⊤diag(νπ ◦π)Φ)−1Φ⊤diag(νπ ◦π) ∈R|S||A|×|S||A| the projection matrix of
function Q ∈R|S||A| onto matrix Φ ∈R|S||A|×d under norm ∥· ∥νπ◦π. We know Qut
∗= Ππt
Φ Qwt.
We have
E[∥QuN −Qut
∗∥2
νt◦πt] = E[(ut
∗−uN)⊤(Φ⊤diag(νt ◦πt)Φ)(ut
∗−uN)]
≤λmax(Φ⊤diag(νt ◦πt)Φ)E[∥ut
∗−uN∥2] = ˜O(1/N).
For the update at step t, θt+1 = RQNPG(θt, ηt, wt, N) and wt = RLTD(πθt, K), let πt = πθt be
the policy at step t. Let ut = uN (uN in Algorithm 3) and Lemma 3 above shows that E[∥Qut −
Qut
∗∥2
νt◦πt] = ˜O( 1
N ). This does not necessarily implies that Qut and Qπt are close since the
property of the critic returned wt is required. We measure the difference between Qut and Qπt in
the following lemma.
Let wt
∗= wπt and wt = RLTD(πθt, K) is the output of RLTD at step t.
Define ϵV,stat :=
maxt=0,1,...,T −1{
q
E[∥Vwt∗−Vwt∥2
νt]}, and ϵV,stat = ˜O(1/
√
K) by Theorem 3.
Lemma 4. Under the conditions in Theorem 5 and Assumption 3, there is some ϵ = ϵstat + ϵbias
that for any π, π′ and any d = dπ′,κπ′
ρ
, dπ∗,κπt
ρ
or νπ∗,
E
h
E(s,a)∼d◦π[Qut(s, a) −Qπt(s, a)]
i ≤ϵ,
(16)
where the outside expectation is taken w.r.t.
the randomness of the data collected in wt =
RLTD(πθt, K) and θt+1 = RQNPG(θt, ηt, wt, N). Moreover, ϵstat =
p
|A| (ξβϵV,stat + ξϵQ,stat)
with ϵV,stat = ˜O(
1
√
K ) and ϵQ,stat = ˜O(
1
√
N ); ϵbias =
p
|A|

ξβ
1−β ϵV,bias + ϵQ,bias

with ϵV,bias
and ϵQ,bias in Definitions 3 and 5, respectively.
Proof of Lemma 4. For any d = dπt+1,κt+1
s
, dπ∗,κt
ρ
or νπ∗and for any π, we know for any Q, Q′
E

E(s,a)∼d◦π[Q(s, a) −Q′(s, a)]
  ≤E
hq
|A|E(s,a)∼d◦Unif[(Q(s, a) −Q′(s, a))2]
i
=
p
|A|E [∥Q −Q′∥d◦Unif] .
To quantify the error between Qut −Qπt, recall Qut
∗= Ππt
Φ Qwt and we decompose it into
Qut −Qπt = (Qut −Qut
∗) + (Ππt
Φ Qwt −Ππt
Φ Qwt∗) + (Ππt
Φ Qwt∗−Ππt
Φ Qπt) + (Ππt
Φ Qπt −Qπt),
and bound each term respectively. We can transfer the norm within the space spanned by Φ via the
assumption that ∥Φu∥d◦Unif ≤ξ∥Φu∥νπ◦π. Note that the first three terms all lie in the span(Φ), we
have the first term bounded by
E
h
∥Qut −Qut
∗∥d◦Unif
i
≤ξE
h
∥Qut −Qut
∗∥νπt◦πt
i
≤ξ
r
E
h
∥Qut −Qut
∗∥2
νπt◦πt
i
≤ξϵQ,stat,
27
for some ϵQ,stat = ˜O(
1
√
N ) as in Lemma 3. The second term is bounded by
E
h
∥Ππt
Φ Qwt −Ππt
Φ Qwt∗∥d◦Unif
i
≤ξE
h
∥Ππt
Φ Qwt −Ππt
Φ Qwt∗∥νπt◦πt
i
≤ξE

∥Qwt −Qwt∗∥νπt◦πt

≤ξβE[∥Vwt −Vwt∗∥νt] ≤ξβ
q
E[∥Vwt −Vwt∗∥2
νt] ≤ξβϵV,stat
for some ϵV,stat = ˜O(
1
√
K ) as in Theorem 5. The third term is bounded by
E
h
∥Ππt
Φ Qwt
∗−Ππt
Φ Qπt∥d◦Unif
i
≤ξE
h
∥Ππt
Φ Qwt∗−Ππt
Φ Qπt∥νπt◦πt
i
≤ξE
h
∥Qwt∗−Qπt∥νπt◦πt
i
≤ξβE[∥Vwt∗−V t∥νπ] ≤ξβ E[∥ΠtV t −V t∥νπ]
1 −β
≤ξβϵV,bias
1 −β
,
where the last inequality is by Definition 3 and inequality (13). The last term is then bounded by
Definition 5 E[∥Ππt
Φ Qπt −Qπt∥d◦Unif] ≤ϵQ,bias. We thus have
E
h
E(s,a)∼d◦π[Qut(s, a) −Qπt(s, a)]
i ≤
p
|A|

ξϵQ,stat + ξβϵV,stat +
ξβ
1 −β ϵV,bias + ϵQ,bias

,
which concludes the proof.
D.2.2
RQNPG One-step Analysis – Mirror Ascent Update
Now we look at the policy improvement of the update θt+1 = RQNPG(θt, ηt, wt, N) with ζn =
Θ(1/n) (Algorithm 3), where wt = RLTD(πθt, K). Let ut = uN (uN in Algorithm 3), and we
know the RQNPG update is θt+1 = θt + ηtut.
This RQNPG update is equivalent to a certain mirror ascent update. Specifically, recall Qut(s, a) =
ϕ(s, a)⊤ut is the approximated robust Q function, the RQNPG update θt+1 = θt+ηtut is equivalent
to [4]
πt+1
s
←arg max
π∈∆A{ηt⟨Qut
s , π⟩−KL(π, πt
s)}
∀s ∈S,
(17)
where we let πs := π(·|s) and Qs := Q(s, ·) for simplicity. Note that this update can be viewed
as a mirror descent step with KL-divergence as Bregman divergence. Given this mirror descent
formulation of policy update in Eq (17), the pushback property indicates that for any policy π (Eq
(2) in [68]),
ηt⟨Qut
s , πt+1
s
⟩−KL(πt+1
s
, πt
s) ≥ηt⟨Qut
s , πs⟩−KL(πs, πt
s) + KL(πs, πt+1
s
),
which is equivalent to the following fundamental inequality
ηt⟨Qut
s , πs −πt
s⟩+ ηt⟨Qut
s , πt
s −πt+1
s
⟩≤KL(π, πt
s) −KL(π, πt+1
s
) −KL(πt+1
s
, πt
s).
(18)
Restatement of Theorem 4 (Approximate policy improvement) For any t ≥0, we know
V πt+1(ρ) ≥V πt(ρ) +
KL
d
πt+1,κπt+1
ρ
(πt, πt+1) + KL
d
πt+1,κπt+1
ρ
(πt+1, πt)
(1 −γ)ηt
−
ϵt
1 −γ ,
(19)
where KLν(π, π′) := P
s ν(s)KL(π(·|s), π′(·|s)) ≥0 and E[ϵt] = ˜O(
1
√
N +
1
√
K ) + O(ϵbias).
Proof of Theorem 4. Let κt+1 = κπt+1 be the worst-case transition kernel w.r.t. policy πt+1, and
let ϵt = Es∼dπt+1,κt+1
ρ
[⟨Qπt
s −Qut
s , πt+1
s
−πt
s⟩]. We have
V πt+1(ρ) −V πt(ρ) ≥
1
1 −γ Es∼dπt+1,κt+1
ρ
[⟨Qπt
s , πt+1
s
−πt
s⟩]
=
1
1 −γ Es∼dπt+1,κt+1
ρ
[⟨Qut
s , πt+1
s
−πt
s⟩] +
1
1 −γ Es∼dπt+1,κt+1
ρ
[⟨Qπt
s −Qut
s , πt+1
s
−πt
s⟩]
28
Algorithm 6: Robust Natural Policy Gradient (RNPG)
Input: θ, η, w, N
Initialize: u0, s0, let π = πθ
for n = 0, 1, . . . , N −1 do
Sample an ∼πθ(·|sn), yn+1 according to p◦
sk,ak and determine sn+1 from y′
n+1
Update un+1 = (1 −λ)un +
ζn∇θ log πθ(an|sn)
h
( ˆT π
P Vw)(sn, an, yn+1) −Vw(sn) −∇θ log πθ(an|sn)⊤un
i
.
// For DS: yn+1 = s′
1:m
i.i.d.
∼p◦
sn,an, sn+1 ∼Unif(yn+1) and ˆT π
P in (3)
// For IPM: yn+1 = sn+1 ∼p◦
sn,an and ˆT π
P in (6)
Return: θ + ηuN
≥
KLdπt+1,κt+1
ρ
(πt, πt+1) + KLdπt+1,κt+1
ρ
(πt+1, πt)
(1 −γ)ηt
−
ϵt
1 −γ ,
where the first inequality is by Lemma 8, and the last inequality is by taking π = πt in the funda-
mental inequality Eq (18), which implies
ηt⟨Qut
s , πt+1
s
−πt
s⟩≥KL(πt
s, πt+1
s
) + KL(πt+1
s
, πt
s).
ϵt can then be bounded by Lemma 4 with E[ϵt] ≤2ϵ = 2ϵstat + 2ϵbias, where ϵstat, ϵbias are
specified in Lemma 4 with ϵstat = ˜O(
1
√
N +
1
√
K ).
D.3
Extension of General Function Approximation of Policy
Setting: In this subsection, we consider a general policy class of form

πθ(a|s) =
exp (fθ(s, a))
P
a′∈A exp (fθ (s, a′)) | θ ∈Rd

,
(20)
where fθ is a differentiable function. This general policy class contains the log-linear policy class
as a special case by fθ(s, a) = ϕ(s, a)⊤θ.
D.3.1
Robust Natural Policy Gradient with General Function Approximation
For the general policy class in Eq (20), we propose a Robust NPG (RNPG) algorithm.
This algorithm can be applied to RNAC (Algorithm 1) for the robust natural actor update. Now we
look at a specific update θt+1 = RNPG(θt, ηt, wt, N), where wt is the output of the robust critic at
step t. Note that for the critic with general function approximation Eq. (14), we slightly abuse the
notation by Vwt = gt, where gt is the output of the FRVE (Algorithm 5) at step t of RNAC. We can
view g ∈G is parameterized by some w, as indicated in the RNAC algorithm Algorithm 1.
Denote by ϕθ(s, a) := ∇θ log πθ(an|sn) and Φθ ∈R|S||A|×d as the feature matrix stacking up
feature vector ϕθ. For each t = 0, 1, . . . , T −1, we let ϕt := ϕθt and Φt := Φθt for simplicity. The
RNPG update is θt+1 ←θt + ηtut, where ut = uN as the output of the stochastic gradient descent
in Algorithm 6,
un+1 = (1 −λ)un + ζn∇θ log πθt(an|sn)
h
( ˆT πθt
P
Vwt)(sn, an, yn+1) −Vwt(sn) −∇θ log πθ(an|sn)⊤un
i
.
RNPG approximates the value approximated advantage function Awt(s, a)
:=
r(s, a) +
γ infp∈Ps,a p⊤Vwt −Vwt(s) by Au
t := Φtu. Note that ( ˆT πt
P Vwt)(sn, an, yn+1) −Vwt(sn) is an
unbiased estimate of Awt(sn, an), i.e.,
E[( ˆT πt
P Vwt)(sn, an, yn+1) −Vwt(sn)|sn, an] = Awt(sn, an).
RNPG thus is iteratively solving the following optimization
1
2∥Awt −Φtu∥2
νt◦πt + λ
2 ∥u∥2,
(21)
29
by stochastic approximation (stochastic gradient descent with Markovian data). Denote by uπt
∗the
optimal value of the optimization, which is also a solution of the equation
0 = (Φt)⊤(Awt −Φtu) −λu
= E(s,a,y′)∼νπ◦π◦p◦
h
ϕt(s, a)

( ˆT πt
P Vwt)(s, a, y′) −Vwt(s) −ϕt(s, a)⊤u
i
−λu.
Theorem 7 (Convergence of compatible advantage function approximation (SGD with Markovian
data)). Under assumption 1, RNPG(θt, ηt, wt, N) (Algorithm 6) with step sizes ζn = Θ(1/n) guar-
antees E[∥uN −ut
∗∥2] = ˜O( 1
N ) and E[∥AuN
t
−Aut
∗
t ∥2
νt◦πt] = ˜O( 1
N ).
Proof of Theorem 7. The theorem can be proved in the same manner as that for Lemma 3, since the
objective function in Eq (21) is strongly convex.
With slight abuse of notation, denote by Σt
ν,π := E(s,a)∼ν◦π

ϕt(s, a)ϕt(s, a)⊤
.
The opti-
mal value ut
∗= (λI + (Φt)⊤diag(νt ◦πt)Φt)−1((Φt)⊤diag(νt ◦πt)Awt) satisfies ∥ut
∗∥≤
∥Awt∥νt◦πt
qPd
i=1 ∥ϕt
i∥2
νt◦πt
λ+λmin(Σt
νt◦πt)
. Let ut = uN, Theorem 7 gives E[∥ut∥2] ≤2E[∥ut
∗∥2] + ˜O( 1
N ). We
then have E[∥ut∥2] ≤U under the following assumption.
Assumption 4 (Bounded feature). Assume supθ
Pd
i=1 ∥ϕθ
i ∥2
νπθ ◦πθ < ∞. Since G (14) is bounded,
∥Awt∥vt◦πt < ∞and there exists 0 < U < ∞that maxt E[∥ut∥2] ≤U.
D.3.2
Robust General Advantage Function Approximation
Denote by Πt
Φt the projection mapping to the space {Φtu : ∥u∥≤∥ut
∗∥} under metric ∥· ∥νt◦πt.
Definition 6. ϵA,bias := maxt=0,1,...,T −1 E [∥Πt
ΦtAt −At∥νπ∗◦π∗].
Note that ϵA,bias also implicitly depends on the choice of λ since ut
∗depends on λ. If the realizable
case, i.e., Aπt ∈span(Φt), ϵA,bias = 0 if λ = 0.
Assumption 5. There is some 0 < ξ′ < ∞that for any θ, supu
∥Φθu∥νπ∗◦π∗
∥Φθu∥νπθ ◦πθ ≤ξ′.
Lemma 5. Under the conditions in Theorems 7 and 6 and Assumption 5, for any t,
E
h
E(s,a)∼νπ∗◦π∗[Aut(s, a) −Aπt(s, a)]
i ≤ϵ′ = ϵ′
stat + ϵ′
bias,
(22)
where the outside expectation is taken w.r.t. the randomness of the data collected the robust natural
actor and robust critic update. Moreover, ϵ′
stat = ˜O(
1
√
K +
1
√
N ); ϵ′
bias = O(ϵA,bias + ϵg,bias) or
O(ϵA,bias + ϵV,bias) for linear robust critic.
Proof of Lemma 5. To quantify the error between Aut −At, we decompose it into
Aut −At = (Aut −Aut
∗) + (Πt
ΦtAwt −Πt
ΦtAwt∗) + (Πt
ΦtAwt∗−Πt
ΦtAt) + (Πt
ΦtAt −At),
and bound each term respectively. The proof follows similarly to that in Lemma 4 by applying
Theorem 7 and Theorem 6.
E
Robust Natural Actor-Critic Analysis
In this section, we first state and prove the formal versions of the main theorems Theorem 1 and
Theorem 2 in Theorem 8 and Theorem 9, respectively. We then give the convergence of RNAC
employing general function approximation in Theorem 10.
30
E.1
Linear Function Approximation
We introduce the detailed setup of the RNAC algorithm with linear function approximation, based
on which the theorems are stated and proved.
RNAC-Linear Setting: We study the RNAC (Algorithm 1) with robust critic performing RLTD
(Algorithm 2 with ξn = Θ(1/k) as in Theorem 5) and robust natural actor performing RQNPG
(Algorithm 3 with ζn = Θ(1/n) as in Lemma 3) under linear value function approximation as in
Eq (12) and log-linear policy as in Eq (15), for DS or IPM uncertainty sets taking δ suggested by
Proposition 4 or Lemma 1, respectively. We assume Assumption 1 and Assumption 3 hold.
For each time t = 0, 1, . . . , T −1, the value approximation and policy in RNAC are updated
as wt = RLTD(πθt, K) and θt+1 = RQNPG(θt, ηt, wt, N), respectively. Denote by ut as uN in
RQNPG(θt, ηt, wt, N) (Algorithm 3), which defines a robust Q-function approximation Qut. Let
πt = πθt, κt = κπt, V t = V πt for simplicity. We have the following lemma.
Lemma 6. Under the RNAC-Linear Setting, for any t = 0, 1, . . . , T −1, we have ⟨Qut
s , πt+1
s
−
πt
s⟩≥0, ∀s ∈S and
E[Es∼ρ[⟨Qut
s , πt
s −πt+1
s
⟩]] ≥E[V t(ρ)] −E[V t+1(ρ)] −
2ϵ
1 −γ ,
where the expectation E is taken w.r.t. the data sampled by RNAC, and ϵ = ϵstat + ϵbias is the same
as that in Lemma 4.
Proof of Lemma 6. Taking π = πt in the fundamental inequality Eq (18) gives
ηt⟨Qut
s , πt
s −πt+1
s
⟩≤−KL(πt
s, πt+1
s
) −KL(πt+1
s
, πt
s) ≤0,
which implies πt+1 is indeed improving πt along Qut direction. We then have
⟨Qut
s , πt
s −πt+1
s
⟩≥
1
1 −γ Es′∼dπt+1,κt+1
s
[⟨Qut
s′ , πt
s′ −πt+1
s′
⟩]
= −
1
1 −γ Es′∼dπt+1,κt+1
s
[⟨Qπt
s′ , πt+1
s′
−πt
s′⟩] +
1
1 −γ Es′∼dπt+1,κt+1
s
[⟨Qut
s′ −Qπt
s′ , πt
s′ −πt+1
s′
⟩]
= −
1
1 −γ Es′∼dπt+1,κt+1
s
,a′∼πt+1
s′ [At(s′, a′)⟩] +
1
1 −γ Es′∼dπt+1,κt+1
s
[⟨Qut
s′ −Qπt
s′ , πt
s′ −πt+1
s′
⟩]
≥V t(s) −V t+1(s) +
1
1 −γ Es′∼dπt+1,κt+1
s
[⟨Qut
s′ −Qπt
s′ , πt
s′ −πt+1
s′
⟩],
where the second inequality is by Lemma 8. The proof of the lemma is then concluded by taking
expectation E[Es∼ρ[·]] on both sides and applying Lemma 4.
Assumption 6. For initial state distribution ρ, there exists M that supκ∈P ∥
d∗,κ
ρ
ρ ∥∞≤M < ∞.
Theorem 8 (Formal statement of Theorem 1). Under RNAC-Linear Setting and Assumption 6,
RNAC with geometrically increasing step sizes ηt ≥
M
1−γ
1−1−γ
M ηt−1 for each t = 1, 2, . . . , T, satisfies
V ∗(ρ) −E[V T (ρ)] ≤

1 −1 −γ
M
T −1 
(1 −1 −γ
M
)(V ∗(ρ) −V 0(ρ)) + log |A|

+

2
1 −γ +
2M
(1 −γ)2

ϵ,
where ϵ = ϵstat + ϵbias as in Lemma 4 with ϵstat = ˜O(
1
√
K +
1
√
N ). Omitting the approximation
ϵbias, the sample complexity for achieving ε robust optimal value (i.e., V ∗(ρ) −E[V T (ρ)] ≤ε) is
˜O(1/ε2) by taking N = K = ˜Θ(1/ε2) and T = Θ(log(1/ε)).
Proof of Theorem 8. Taking π = π∗in inequality (18), we have
⟨Qut
s , π∗
s −πt
s⟩+ ⟨Qut
s , πt
s −πt+1
s
⟩≤1
ηt KL(π∗
s, πt
s) −1
ηt KL(π∗
s, πt+1
s
).
(23)
31
We then take the expectation E[Es∼d∗,κt
ρ
[·]] on both sides. Note that
E[Es∼d∗,κt
ρ
[⟨Qut
s , π∗
s −πt
s⟩]] = E[Es∼d∗,κt
ρ
[⟨Qπt
s , π∗
s −πt
s⟩]] + E[Es∼d∗,κt
ρ
[⟨Qut
s −Qπt
s , π∗
s −πt
s⟩]]
≥(1 −γ)(V ∗(ρ) −E[V t(ρ)]) −2ϵ,
where the inequality is by Lemma 8 and Lemma 4. Then by Lemma 6 and Assumption 6 that
M = supκ ∥
d∗,κ
ρ
ρ ∥∞, we have
E[Es∼d∗,κt
ρ
[⟨Qut
s , πt
s −πt+1
s
⟩]] ≥M

E[V t(ρ)] −E[V t+1(ρ)] −
2ϵ
1 −γ

,
since ⟨Qut
s , πt
s −πt+1
s
⟩≤0. The outside expectation E is taken w.r.t. the data sampled by RNAC,
and since all the following statements are under expectation E, we omit E in the proof for simplicity
and bring it back at the end. Combining the inequality above and Eq (23), we have
(1 −γ)(V ∗(ρ) −V t(ρ)) −2ϵ + M

V t(ρ) −V t+1(ρ) −
2ϵ
1 −γ

≤1
ηt KLd∗,κt
ρ
(π∗, πt) −1
ηt KLd∗,κt
ρ
(π∗, πt+1).
It follows that
V ∗(ρ) −V t+1(ρ) ≤

1 −1 −γ
M

(V ∗(ρ) −V t(ρ)) +
1
Mηt KLd∗,κt
ρ
(π∗, πt)
−
1
Mηt KLd∗,κt
ρ
(π∗, πt+1) + 2
 ϵ
M +
ϵ
1 −γ

,
which implies
V ∗(ρ) −V T (ρ) ≤

1 −1 −γ
M
T
(V ∗(ρ) −V 0(ρ)) +

1 −1 −γ
M
T −1 KLd∗κ0
ρ
(π∗, π0)
Mη0
+
T −1
X
t=1

1 −1 −γ
M
T −1−t  
1
Mηt KLd∗,κt
ρ
(π∗, πt) −1 −1−γ
M
Mηt−1 KLd∗,κt−1
ρ
(π∗, πt)
!
+ 2M
1 −γ
 ϵ
M +
ϵ
1 −γ

.
Take geometrically increasing step sizes ηt ≥
M
1−γ
1−1−γ
M ηt−1 for each t. Since M = supκ ∥
d∗,κ
ρ
ρ ∥∞≥
(1 −γ) supκ,κ′ ∥
d∗,κ
ρ
d∗,κ′
ρ
∥∞, we have
1
Mηt KLd∗,κt
ρ
(π∗, πt) −1 −1−γ
M
Mηt−1 KLd∗,κt−1
ρ
(π∗, πt)
≤1 −1−γ
M
Mηt−1
1 −γ
M
KLd∗,κt
ρ
(π∗, πt) −KLd∗,κt−1
ρ
(π∗, πt)

≤0.
We bring back the expectation E over the data and finally arrive at
V ∗(ρ) −E[V T (ρ)] ≤

1 −1 −γ
M
T
(V ∗(ρ) −V 0(ρ)) +

1 −1 −γ
M
T −1 KLd∗κ0
ρ
(π∗, π0)
Mη0
+ 2M
1 −γ
 ϵ
M +
ϵ
1 −γ

,
where KLd∗κ0
ρ
(π∗, π0) ≤log |A| when π0 is a uniform policy.
Theorem 9 (Formal statement of Theorem 2). Under RNAC-Linear Setting and Assumption 6 when
IPM uncertainty set is considered, take ρ = νπ∗and ηt = η = (1 −γ) log(|A|), RNAC satisfies
V ∗(ρ) −1
T
T
X
t=1
E[V t(ρ)] ≤
2
(1 −γ)(1 −β)T +
2
1 −β
2 −γ
1 −γ ϵ,
32
where β is in Assumption 6 (guaranteed by Proposition 4 for DS uncertainty set and assumed for
IPM uncertainty set), and ϵ = ϵstat +ϵbias as in Lemma 4 with ϵstat = ˜O(
1
√
K +
1
√
N ). Omitting the
approximation ϵbias, the sample complexity for achieving ε robust optimal value on average (i.e.,
V ∗(ρ) −1
T
PT
t=1 E[V t(ρ)] ≤ε) is ˜O(1/ε3) by N = K = ˜Θ(1/ε2) and T = Θ(1/ε).
Proof of Theorem 9. Taking π = π∗in inequality (18), we have
⟨Qut
s , π∗
s −πt
s⟩+ ⟨Qut
s , πt
s −πt+1
s
⟩≤1
η KL(π∗
s, πt
s) −1
η KL(π∗
s, πt+1
s
).
Since all the following statements are under expectation E over the data, we omit E in the proof for
simplicity and bring it back at the end. According to Lemma 9 and Lemma 4, we have
Es∼ρ[⟨Qut
s , π∗
s −πt
s⟩] = Es∼ρ[⟨Qπt
s , π∗
s −πt
s⟩] + Es∼ρ[⟨Qut
s −Qπt
s , π∗
s −πt
s⟩]
≥(1 −β)(V ∗(ρ) −V t(ρ)) −2ϵ.
Based on Lemma 6, we have
Es∼ρ[⟨Qut
s , πt
s −πt+1
s
⟩] ≥V t(ρ) −V t+1(ρ) −
2ϵ
1 −γ .
It then follows that
(1 −β)(V ∗(ρ) −V t+1(ρ)) ≤V t+1(ρ) −V t(ρ) + KLρ(π∗, πt) −KLρ(π∗, πt+1)
η
+ 2 −γ
1 −γ 2ϵ.
Taking summation from t = 0 to T −1 from both sides we have
V ∗(ρ) −1
T
T
X
t=1
V t(ρ) ≤
V T (ρ)
(1 −β)T + KLρ(π∗, π0)
η(1 −β)T
+
2ϵ
1 −β
2 −γ
1 −γ .
We then conclude the theorem by choosing η = (1 −γ) log(|A|) and bringing back the expectation
E over data,
V ∗(ρ) −1
T
T
X
t=1
E[V t(ρ)] ≤
2
(1 −γ)(1 −β)T +
2
1 −β
2 −γ
1 −γ ϵ.
Discussion:
The linear convergence for NPG with linear function approximation has been previ-
ously studied in canonical MDP [4]. We present the linear convergence of RNAC (Algorithm 1) in
Theorem 8. Compared to Theorem 8, Theorem 9 utilizing constant step sizes and leads to sublinear
convergence, and it is proved only for initial state distribution ρ = νπ∗. Moreover, Theorem 9 does
not require Assumption 6 though may need Assumption 2 for IPM uncertainty set.
E.2
General Function Approximation
RNAC-General Setting. We study the RNAC (Algorithm 1) with robust critic performing FRVE
(Algorithm 5) and robust natural actor performing RNPG (Algorithm 6 with ζn = Θ(1/n) as in
Theorem 7) under general value function approximation as in Eq (14) and general policy class as
in Eq (20), for DS uncertainty set taking δ suggested by Proposition 4. We assume Assumption 1,
Assumption 5 and Assumption 4 (∥ut
∗∥≤U), hold.
Theorem 10. Under RNAC-General Setting, suppose log πθ(a|s) is an L-smooth function of θ.
Take ρ = νπ∗and ηt = η =
q
2KLρ(π∗,π0)
LU 2T
for each t, RNAC satisfies
V ∗(ρ) −E
"
1
T
T −1
X
t=0
V t(ρ)
#
≤
p
2LU 2KLρ(π∗, π0)
(1 −β)
√
T
+
ϵ′
1 −β ,
where β is in Assumption 6 (guaranteed by Proposition 4 for DS uncertainty set), and ϵ′ = ϵ′
stat +
ϵ′
bias as in Lemma 5 with ϵ′
stat = ˜O(
1
√
K +
1
√
N ). Omitting the approximation ϵ′
bias, the sample
complexity for achieving ε robust optimal value is ˜O(1/ε4) by N = K = ˜Θ(1/ε2) and T =
Θ(1/ε2).
33
Proof. We have
(1 −β)(V ∗(ρ) −E[V t(ρ)]) ≤E[E(s,a)∼ρ◦π∗[At(s, a)]]
= E[E(s,a)∼ρ◦π∗[∇θ log πt(a|s)⊤ut] + E[E(s,a)∼ρ◦π∗[At(s, a) −Aut(s, a)]]
≤E

E(s,a)∼ρ◦π∗
1
η log πt+1(a|s)
πt(a|s)

+ ηLE[∥ut∥2]
2
+ ϵ′
≤1
η E[KLρ(π∗, πt) −KLρ(π∗, πt+1)] + ηLU 2
2
+ ϵ′,
where the first inequality is by Lemma 9, the first equality is by the definition of Aut(s, a) =
∇log πt(a|s)⊤ut, and the second inequality is by the L-smoothness of log πθ(a|s) and Lemma 5.
We then conclude the proof by summing from t = 0 to T −1 from both sides and taking η =
q
2KLρ(π∗,π0)
LU 2T
.
Note that in RNAC-General Setting, the critic is employing general function approximation, where
the IPM uncertainty set is not defined (c.f. Section C.2 for more discussion). It is not hard to show
a similar result as in Theorem 10 for the RNAC with robust critic performing RLTD (Algorithm 2)
and robust natural actor performing RNPG (Algorithm 6) for both DS and IPM uncertainty sets.
F
Supporting Lemmas
We introduce some supporting lemmas for proving the results of this paper.
F.1
Performance Difference Lemmas
A key supporting lemma in the proof of convergence of policy gradient-based methods is the perfor-
mance difference lemma. In the canonical RL with a fixed transition kernel κ, we have the following
performance lemma.
Lemma 7 (Performance difference [23]). For any policy π, π′, and transition κ, we have
V π′
κ (ρ) −V π
κ (ρ) =
1
1 −γ Es∼dπ′,κ
ρ
Ea∼π′(·|s)[Aπ(s, a)].
However, due to the non-singleton uncertainty set P, the worst-case transition kernel is a function
of policy π, and we have the following performance difference inequality lemma.
Lemma 8 (Robust performance difference). For any policy π, π′, denote κ′ = κπ′ and κ = κπ be
their worst-case transition kernels, respectively. Then
1
1 −γ Es∼dπ′,κ′
ρ
Ea∼π′(·|s)[Aπ(s, a)] ≤V π′(ρ) −V π(ρ) ≤
1
1 −γ Es∼dπ′,κ
ρ
Ea∼π′(·|s)[Aπ(s, a)].
Proof of Lemma 8. The LHS is by
V π′
κ′ (ρ) −V π(ρ) = Eκ′,π′

X
t≥0
γt(r(st, at) + V π(st) −V π(st))

−V π(ρ)
= Eκ′,π′

X
t≥0
γt(r(st, at) + γV π(st+1) −V π(st))


≥
1
1 −γ Es∼dπ′,κ′
ρ
Ea∼π′(·|s)[Aπ(s, a)].
The RHS is by
V π′
κ′ (ρ) −V π
κ (ρ) ≤V π′
κ (ρ) −V π
κ (ρ) =
1
1 −γ Es∼dπ′,κ
ρ
Ea∼π′(·|s)[Aπ(s, a)].
34
Moreover, the optimality gap between the optimal policy π∗and any policy π is upper bounded as
in the lemma below.
Lemma 9 (Robust optimality gap lemma). Under Assumption 2, for any π and any ρ,
V ∗(ρ) −V π(ρ) ≤
1
1 −β Es∼dπ∗,p◦
ρ
,a∼π∗s [Aπ(s, a)].
Proof of Lemma 9.
V ∗(ρ) −V π(ρ) = Es∼ρEa∼π∗s [r(s, a) + γ
X
s′
κπ∗(s′|s, a)V ∗(s′)] −V π(ρ)
= Es∼ρEa∼π∗[r(s, a) + γ
X
s′
κπ(s′|s, a)V π(s′) −V π(ρ)]
+ γEs∼ρEa∼π∗
X
s′
(κπ∗(s′|s, a)V ∗(s′) −κπt(s′|s, a)V π(s′))
≤Es∼ρEa∼π∗[Aπ(s, a)] + γEs∼ρEa∼π∗
X
s′
κπt(s′|s, a)(V ∗(s′) −V π(s′))
≤Es∼ρEa∼π∗[Aπ(s, a)] + βEs∼ρEa∼π∗
X
s′
p◦
s,a(s′)(V ∗(s′) −V π(s′))
≤· · ·
≤
1
1 −β Es∼dπ∗,p◦
ρ
,a∼π∗
s [Aπ(s, a)].
F.2
Concentration Lemmas with Markov samples
Assumption 7 (Uniformly geometric mixing, quantitative restatement of Assumption 1). There
exists some 0 < λ < 1 and C > 0 such that for any policy π, the Markov chain {sk} induced by
applying π in the nominal model p◦is geometrically ergodic with unique stationary distribution νπ,
i.e., maxs ∥P(sk ∈·|s0 = s) −νπ∥T V ≤Cλk, ∀k. The uniform convergence to unique stationary
distribution implies that for any policy π the spectral gap (i.e., the difference between the largest
and the second largest eigenvalues) of the state transition kernel has a strictly positive lower bound.
Lemma 10 (Bernstein’s inequality for stationary Markovian data, Theorem 2 in [21]). Suppose
Z1, Z2, . . . , Zn follow some stationary Markovian data in space Z with stationary distribution ν ∈
∆Z and spectral gap of the Markov chain strictly greater than 0. For any function f : Z →[−c, c]
that, EZ∼ν[f(Z)] = 0 and EZ∼ν[f(Z)2] = σ2, with probability at least 1 −δ,
1
n
n
X
i=1
f(Zi) ≤C1
r
σ2 ln(1/δ)
n
+ C2
c ln(1/δ)
n
,
for some spectral gap-dependent multiplicative factors C1, C2.
Since we assume Assumption 7 throughout the paper during the analysis, the spectral gap is uni-
formly lower bounded by some constant great than zero, and we view C1, C2 as some constant that
can be omitted in the big-O notation. We refer to [21] for further details of Bernstein’s inequality
for stationary Markovian data.
Lemma 11 (Lemma A.11 in [2] with Markovian data). Consider a stationary Markov chain on
domain X × Z that is geometrically mixing with transition p(x′, z′|x, z) = pX′|Z(x′|z)pZ|X(z′|x)
and stationary distribution ν ◦pZ|X. Let D = {(X1, Z1), (X2, Z2), . . . , (Xn, Zn)} be station-
ary Markovian data sampled with X1 ∼ν.
Given a function class G ⊂RX and a func-
tion G : X × Z →R that EZ∼pZ|X(·|x)[G(x, Z)] = g∗(x) and |G(x, z)| ≤Gmax.
Let
ˆg = arg ming∈G 1
2
P
(x,z)∈D(G(x, z) −g(x))2 be the MSE estimate of g∗within G, then with
probability at least 1 −δ,
r
Ex∼ν
h
(ˆg(x) −g∗(x))2i
≤O
 
Gmax
r
log(|G|/δ)
n
!
+ O(ϵbias),
(24)
35
where ϵbias = ming∈G ∥g −g∗∥ν.
Proof. The proof follows the same as [2]. The only difference is replacing the canonical Bernstein’s
inequality with i.i.d. data by Bernstein’s inequality for stationary Markovian data in Lemma 10.
F.3
Convergence of Stochastic Approximation
F : Rd × Z →Rd is a stochastic operator. The stochastic approximation algorithm updates the
parameter by wk+1 = wk + αkF(w, Zk), where (Z0, Z1, . . .) is a Markov chain, that is geometri-
cally ergodic for some C > 0, 1 > λ > 0 with stationary distribution ν, i.e., maxz ∥P(Zk ∈·|Z0 =
z) −ν∥T V ≤Cλk, ∀k. (c.f. Assumption 1). The stochastic approximation algorithm solves the
following equation
0 = ¯F(w) := EZ∼ν[F(w, Z)],
under the following assumption and appropriate step sizes (α1, α2, . . .).
Assumption 8.
1. (Z0, Z1, . . .) are geometrically mixed to the stationary distribution.
2. ∃L1 > 0 that ∥F(w, z) −F(w′, z)∥≤L1∥w −w′∥, ∥F(0, z)∥≤L1 for any w, w′ ∈
Rd, z ∈Z.
3. ¯F(w) = 0 has unique solution w∗and there exists c0 > 0 that (w −w∗)⊤¯F(w) ≤
−c0∥w −w∗∥2 for any w ∈Rd.
The second condition of Assumption 8 is a Lipschitz condition of the operator F, and the third
condition is the “strongly-concavity” structure of the averaged operator ¯F.
Lemma 12 (Big-O version of Corollary 2.1.2 in [10]). Under Assumption 8, taking appropriate
αk = Θ(1/k) gives E[∥wK −w∗∥2] = ˜O(1/K).
To apply the lemma above, we need to show that the proposed operator F satisfies Assumption 8.
G
More Related Works
The framework of the robust Markov decision process (RMDP) is proposed by [19, 40] to learn the
optimal robust policy that achieves the optimal worst-case performance over all possible models in
the uncertainty set. For the planning problem of RMDP, Iyengar [19] and Nilim et al. [40] show that
value iteration achieves linear convergence to the optimal robust values.
When the transition model is unknown, several model-based and value-based approaches have been
proposed and studied for robust RL in both the tabular setting and the function approximation set-
ting. Under the tabular setting, Xu et al. [64] design the model-based robust phase value learning
algorithm and demonstrates an O(1/ϵ2) sample complexity bound. Under the function approx-
imation setting, Panaganti et al. [43] develop a robust fitted Q-iteration with the total variation
uncertainty set and show an O(1/ϵ2) sample complexity to achieve approximate optimality and [6]
develop a model-based method with the same sample complexity under certain coverage assump-
tion. However, these methods are not scalable to continuous control problems. Instead, policy-based
approaches directly parameterize policy and have more representation power in modeling stochas-
tic policies, more flexibility for policy manipulation, and a better ability to solve robotics control
problems with large action space.
The existing policy-based methods with convergence guarantees mainly focus on the tabular setting.
Wang et al. [61] show an O(1/ϵ7) sample complexity for robust actor-critic with R-contamination
uncertainty sets. Li et al. [29] prove an ˜O(1/ϵ2) sample complexity for robust natural actor-critic
assuming the existence of an oracle to solve the inner optimization problem. Kumar et al. [27] de-
velop a robust policy gradient method based on ℓp norm uncertainty sets and extends its formulation
under the s-rectangular assumption. All works mentioned above are limited to the tabular case with
critic Q and adopt computationally infeasible uncertainty sets in the case of the large state space.
Kuang et al. [26] illustrate a state disturbance view of Wasserstein metric-based RMDP, which can
be generalized to the large state space. However, they only provide a policy iteration guarantee in
the tabular setting (contraction under ∥· ∥∞). Instead, we focus on large-scale robust RL, propose
36
two computationally efficient uncertainty sets, and demonstrate an ˜O(1/ϵ2) (resp., ˜O(1/ϵ4)) sam-
ple complexity under linear (resp., general) function approximation. Additionally, we regard V as
a critic instead of Q, which is closer to the actual implementation of on-policy natural actor-critic
algorithms.
37
