paper_id,claim_sentence,csv_score,ebv_score,cds_score,severity
0073cc73e1873b35345209b50a3dab66-Paper-Conference,"Extensive experiments
have demonstrated the effectiveness of our method for large-scale scene novel view
synthesis, which outperforms relevant state-of-the-art baselines.",0.6,0.85,-0.25,Strongly Supported
0073cc73e1873b35345209b50a3dab66-Paper-Conference,"We summarize the contributions
as follows: 1) Aiming at novel view synthesis for large outdoor scenes, we propose an implicit
neural representation framework based on point diffusion models to provide dense surface priors to
cope with the exploding sampling space.",0.3,0.85,-0.55,Strongly Supported
0073cc73e1873b35345209b50a3dab66-Paper-Conference,"3) Extensive experiments demonstrate that our PDF network outperforms state-of-the-art methods,
including robustness to large-scale outdoor scene representation and the capability to synthesize more
photo-realistic novel views.",0.6,0.85,-0.25,Strongly Supported
0073cc73e1873b35345209b50a3dab66-Paper-Conference,"We compare our method with the previous state-of-the art
methods on novel view synthesis, including NeRF [19], NeRF++ [41], Mip-NeRF [1], Mip-NeRF
360 [2], Mega-NeRF [28], Ref-NeRF [29].",0.3,0.85,-0.55,Strongly Supported
0073cc73e1873b35345209b50a3dab66-Paper-Conference,"We outperform other methods on all average evaluation
metrics, especially LPIPS, a perceptual metric close to the human visual system, which is significantly
more sensitive to the foreground than the background.",0.5000000000000001,0.85,-0.34999999999999987,Strongly Supported
0073cc73e1873b35345209b50a3dab66-Paper-Conference,"Our method exhibits superior
performance compared to the GAN-based point cloud up-sampling method, primarily due to its
9
Table 3: Quantitative results of ablation experiments, including removing the diffusion-based point
cloud up-sampling module, using the GAN-based point cloud up-sampling method, our PDF method.",0.4666666666666667,0.85,-0.3833333333333333,Strongly Supported
0073cc73e1873b35345209b50a3dab66-Paper-Conference,"5
Conclusions and Limitations
In this paper, we propose PDF, a point diffusion implicit function for large-scale scene neural
representation, and demonstrate its robustness and fidelity on novel view synthesis tasks.",0.4333333333333333,0.85,-0.4166666666666667,Strongly Supported
0073cc73e1873b35345209b50a3dab66-Paper-Conference,"Extensive experiments demonstrate that our method
outperforms current methods in both subjective and objective aspects.",0.6,0.85,-0.25,Strongly Supported
00296c0e10cd24d415c2db63ea2a2c68-Paper-Conference,"In the context of ℓ2-regularized M-estimation problem,
we propose a novel grid point selection scheme and an adaptive stopping criterion
for any given optimization algorithm that produces an approximated solution path
with approximation error guarantee.",0.5333333333333333,0.65,-0.1166666666666667,Aligned
00296c0e10cd24d415c2db63ea2a2c68-Paper-Conference,"In the context of ℓ2-regularized M-estimation where p(θ) = (1/2)∥θ∥2
2, we propose a novel grid
point selection scheme and an associated stopping criterion at each grid point for any optimization
method.",0.3,0.65,-0.35000000000000003,Strongly Supported
00296c0e10cd24d415c2db63ea2a2c68-Paper-Conference,"4
Motivated by this observation and the local approximation-error bounds in Theorem 1, we propose a
novel stopping criterion scheme at the selected grid points for any general optimization algorithm.",0.3,0.65,-0.35000000000000003,Strongly Supported
00296c0e10cd24d415c2db63ea2a2c68-Paper-Conference,"Formally, we show that, given any ϵ > 0, the above scheme achieves ϵ-suboptimality (11) for ˜θ(t)
(up to a multiplicative constant).",0.4333333333333333,0.65,-0.21666666666666673,Strongly Supported
00296c0e10cd24d415c2db63ea2a2c68-Paper-Conference,"5
Discussion
In this article, we proposed a novel grid point selection scheme and stopping criterion for any general
path-following algorithms.",0.3,0.65,-0.35000000000000003,Strongly Supported
00bb4e415ef117f2dee2fc3b778d806d-Paper-Conference,"Our method significantly improves performance on synthetic and real directed coupling
networks using the data generated by four well-known neural dynamic models, and the
number of labels required is very small compared to the size of the coupled networks.",0.3666666666666667,0.95,-0.5833333333333333,Strongly Supported
00bb4e415ef117f2dee2fc3b778d806d-Paper-Conference,"4.2
Ablation study
Our method’s innovation can be decomposed into three components: Firstly, a trainable transfer
entropy calculator (TENE) is proposed; Secondly, TENE is combined with attention to detect region
of high transfer entropy; Thirdly, the classifier is guided by coupling attention to focus on regions
where the coupling effect may emerge.",0.2666666666666666,0.95,-0.6833333333333333,Strongly Supported
00bb4e415ef117f2dee2fc3b778d806d-Paper-Conference,"Compared with the baselines, our method usually
substantially improves reconstruction performance on real coupled networks, as shown in Table 3.",0.4666666666666667,0.95,-0.4833333333333333,Strongly Supported
00bb4e415ef117f2dee2fc3b778d806d-Paper-Conference,"5
Limitations
Our methodology has limitations (i.e., cases for which performance improvement is less): 1.",0.5333333333333333,0.95,-0.41666666666666663,Strongly Supported
001608167bb652337af5df0129aeaabd-Paper-Conference,"For our methods on
RL settings, we compute the maximum success rate averaged across a sliding window over all test
episodes to account for in-context improvement.",0.5333333333333333,1.0,-0.4666666666666667,Strongly Supported
001608167bb652337af5df0129aeaabd-Paper-Conference,"4.1
Main Evaluations
We answer the first two questions above by comparing learned agents from our method against 1)
Reinforcement Learning (RL) oracles in online RL settings and 2) well-established baselines on
learning from mixed-quality demonstrations in the Imitation Learning (IL) setting.",0.3,1.0,-0.7,Strongly Supported
001608167bb652337af5df0129aeaabd-Paper-Conference,"On average,
our method surpasses the concurrent AT baseline and achieves significantly better performance than
other baselines (Table A.6).",0.3333333333333333,1.0,-0.6666666666666667,Strongly Supported
001608167bb652337af5df0129aeaabd-Paper-Conference,"The underlying hypothesis behind our method is
that cross-episodic attention enables Transformer agents to distill policy improvement when
mixed-optimality trajectories are viewed collectively.",0.5333333333333333,1.0,-0.4666666666666667,Strongly Supported
001608167bb652337af5df0129aeaabd-Paper-Conference,"Unlike AD, which focuses on in-context improvements at test time and requires numerous
single-task source agents for data generation, our approach improves data efficiency for Transformer
agents by structuring data in curricula, requiring only a single multi-task agent and allowing for
diverse task instances during evaluations.",0.5333333333333333,1.0,-0.4666666666666667,Strongly Supported
007f4927e60699392425f267d43f0940-Paper-Conference,"To
this end, we propose two novel uncertainty set formulations, one based on double
sampling and the other on an integral probability metric.",0.3,0.36,-0.06,Aligned
007f4927e60699392425f267d43f0940-Paper-Conference,"We propose two novel uncertainty sets, one using double sampling
(Section 3.1) and the other an integral probability metric (Section 3.2), which are both compatible
for robust RL with large state space and function approximation.",0.3,0.36,-0.06,Aligned
007f4927e60699392425f267d43f0940-Paper-Conference,"We propose a novel RNAC algorithm (Section 4) which to the best of our knowledge is the
first policy-based approach for robust RL under function approximation, with provable convergence
guarantees.",0.5333333333333333,0.36,0.17333333333333334,Moderate Divergence
0001ca33ba34ce0351e4612b744b3936-Paper-Conference,"We propose two improvements to the mean-ﬁeld inference scheme that aim to more faithfully invert
the SAMS-VAE generative model.",0.5333333333333333,1.0,-0.4666666666666667,Strongly Supported
0001ca33ba34ce0351e4612b744b3936-Paper-Conference,"We note that CPA-VAE inherits the beneﬁts
of the inference improvements to the variational families we propose in this work and will assess the
contributions of better inference and sparse masking separately.",0.5333333333333333,1.0,-0.4666666666666667,Strongly Supported
00db17c36b5435195760520efa96d99c-Paper-Conference,"For the first time, we propose a sound notion of adversarial robustness
that accounts for task equivariance.",0.3,0.833,-0.5329999999999999,Strongly Supported
00db17c36b5435195760520efa96d99c-Paper-Conference,"Euclidean isometries for ℓ2)
Center
36
F
Graph edit distance certificates
In the following, we show how the different existing approaches for proving the robustness of graph
neural networks operating on {0, 1}N×D × {0, 1}N×N w.r.t to distance din
c+
X · ||(X′ −X)+||0 + c−
X · ||(X′ −X)−||0 + c+
A · ||(A′ −A)+||0 + c−
A · ||(A′ −A)−||0, (4)
with costs c+
X, c−
X, c+
A, c−
A ∈{∞, 1} can be generalized to non-uniform costs, i.e.",0.3333333333333333,0.833,-0.49966666666666665,Strongly Supported
00db17c36b5435195760520efa96d99c-Paper-Conference,"We propose to compute the lower and upper bounds R(0) and S(0) for the first pre-activation values
by solving a knapsack problem with local constraints (see Definition 2) via the methods discussed
in Appendix F.1.",0.3,0.833,-0.5329999999999999,Strongly Supported
002262941c9edfd472a79298b2ac5e17-Paper-Conference,"Bootstrapping Vision-Language Learning with
Decoupled Language Pre-training
Yiren Jian1
Chongyang Gao2
Soroush Vosoughi1
1Dartmouth College
2Northwestern University
Abstract
We present a novel methodology aimed at optimizing the application of frozen large
language models (LLMs) for resource-intensive vision-language (VL) pre-training.",0.3,0.85,-0.55,Strongly Supported
002262941c9edfd472a79298b2ac5e17-Paper-Conference,"3.2
Preliminary: BLIP-2 forward-decoupled training
While our proposed framework is flexible in regards to the specific architecture of Θ
V→L or the learning
strategy deployed, for illustrative purposes, we employ BLIP-2 as a case study to demonstrate the
applicability of our approach with state-of-the-art learning methods, owing to the strong performance
and reproducibility of BLIP-2.",0.6666666666666666,0.85,-0.18333333333333335,Aligned
002262941c9edfd472a79298b2ac5e17-Paper-Conference,"The first
stage of training involves pre-training the Q-Former with Equation 7 (LBLIP2-stage1 ≡ITC + ITM +
ITG), supplemented with the alignment loss introduced by the P-Former, as defined in Equation 6:
LBLIP2-stage1 + ω1 × Lalignment
(8)
Subsequently, the second stage of training, in line with our approach, involves BLIP-2’s stage 2,
which is the end-to-end training of Equation 1: LBLIP2-stage2 ≡L(Dlanguage( Θ
V→L(Evision(I))), t)),
again enhanced with the alignment loss imparted by P-Former in Equation 6:
LBLIP2-stage2 + ω2 × Lalignment
(9)
Figure 3 provides a schematic representation of the proposed integration of our framework and
P-Former with BLIP-2.",0.3,0.85,-0.55,Strongly Supported
002262941c9edfd472a79298b2ac5e17-Paper-Conference,"Despite being a smaller-scale experiment without large-scale pre-training, we demonstrate that
our learning framework can be generalized to another modality (i.e., video-learning), employing a
different vision-language adaptor (i.e., a plain Transformer as opposed to a Q-Former).",0.3333333333333333,0.85,-0.5166666666666666,Strongly Supported
002262941c9edfd472a79298b2ac5e17-Paper-Conference,"However, as evidenced in Table 4 (ω1 = 0 and ω2 = 100), our approach achieves competitive results
even without the alignment loss in stage-1, focusing the alignment solely on stage-2.",0.3,0.85,-0.55,Strongly Supported
0021c2cb1b9b6a71ac478ea52a93b25a-Paper-Conference,"• We conduct extensive experiments for analysis and comparisons on seven different image
segmentation datasets, the results of which show the superiority and generalization ability
of our methods.",0.6,0.925,-0.32500000000000007,Strongly Supported
0021c2cb1b9b6a71ac478ea52a93b25a-Paper-Conference,"As shown in Table 2, we compare our method with state-of-the-art point prompt
segmentation approaches.",0.6666666666666666,0.925,-0.2583333333333334,Strongly Supported
0021c2cb1b9b6a71ac478ea52a93b25a-Paper-Conference,"We notice that our method can significantly outperform it in terms of IoU,
with improvements of 8.2, 6.8, and 16.1 on GrabCut, Berkeley, and DAVIS correspondingly.",0.3666666666666667,0.925,-0.5583333333333333,Strongly Supported
